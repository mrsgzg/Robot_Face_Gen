#!/usr/bin/env python3
"""
é‡å çª—å£ï¼ˆæ»‘åŠ¨çª—å£ï¼‰æ¨ç†æµ‹è¯•
å®ç°é€å¸§è¾“å‡ºçš„æµå¼æ¨ç†
"""

import os
import time
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import argparse
from tqdm import tqdm
from collections import deque

from facial_reaction_model_old.model import FacialReactionModel
from Data_Set_old import SpeakerListenerDataset


class SlidingWindowTester:
    """é‡å çª—å£æ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.checkpoint_path = checkpoint_path
        
        # æ»‘åŠ¨çª—å£çŠ¶æ€
        self.window_size = None
        self.past_reaction_features = None
        self.past_motion_sample = None
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_times = []
        self.frame_predictions = []
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {self.checkpoint_path}")
        
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        config = checkpoint.get('config', {})
        model_config = config.get('model', {})
        
        self.model = FacialReactionModel(
            feature_dim=model_config.get('feature_dim', 256),
            audio_dim=model_config.get('audio_dim', 384),
            period=model_config.get('period', 25),
            max_seq_len=model_config.get('max_seq_len', 750),
            device=str(self.device),
            window_size=model_config.get('window_size', 16),
            momentum=model_config.get('momentum', 0.9)
        )
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        self.window_size = self.model.window_size
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œçª—å£å¤§å°: {self.window_size}")
        
    def reset_states(self, batch_size=1):
        """é‡ç½®æ»‘åŠ¨çª—å£çŠ¶æ€"""
        self.past_reaction_features = torch.zeros(
            (batch_size, self.window_size, 158),  # 136+17+3+2
            device=self.device
        )
        self.past_motion_sample = None
        
    def sliding_window_inference(self, sample_data, stride=1, max_frames=None):
        """
        æ‰§è¡Œæ»‘åŠ¨çª—å£æ¨ç†
        Args:
            sample_data: æµ‹è¯•æ ·æœ¬æ•°æ®
            stride: æ»‘åŠ¨æ­¥é•¿ï¼ˆ1=é€å¸§ï¼Œ4=æ¯4å¸§ï¼Œ16=éé‡å ï¼‰
            max_frames: æœ€å¤§å¤„ç†å¸§æ•°
        """
        print(f"ğŸš€ å¼€å§‹æ»‘åŠ¨çª—å£æ¨ç† (æ­¥é•¿: {stride})")
        
        speaker_data = sample_data['speaker']
        listener_data = sample_data['listener']
        total_frames = speaker_data['landmarks'].shape[0]
        
        if max_frames:
            total_frames = min(total_frames, max_frames)
        
        print(f"  æ€»å¸§æ•°: {total_frames}")
        print(f"  çª—å£å¤§å°: {self.window_size}")
        print(f"  é¢„æœŸè¾“å‡ºå¸§æ•°: {max(0, total_frames - self.window_size + 1)}")
        
        # é‡ç½®çŠ¶æ€
        self.reset_states()
        self.inference_times.clear()
        self.frame_predictions.clear()
        
        # æ»‘åŠ¨çª—å£æ¨ç†
        progress_bar = tqdm(
            range(self.window_size, total_frames + 1, stride),
            desc="æ»‘åŠ¨çª—å£æ¨ç†"
        )
        
        for end_frame in progress_bar:
            start_frame = end_frame - self.window_size
            
            # å‡†å¤‡å½“å‰çª—å£æ•°æ®ï¼ˆåŒ…å«å†å²contextï¼‰
            window_speaker = {}
            for feature_name in ['landmarks', 'au', 'pose', 'gaze', 'audio']:
                if feature_name in speaker_data:
                    # æä¾›ä»å¼€å§‹åˆ°å½“å‰çš„æ‰€æœ‰æ•°æ®ï¼Œæ¨¡å‹å†…éƒ¨ä¼šæˆªå–æœ€åwindow_size
                    feature_data = speaker_data[feature_name][:end_frame]
                    window_speaker[feature_name] = feature_data.unsqueeze(0).to(self.device)
            
            # æ¨ç†
            start_time = time.time()
            
            with torch.no_grad():
                current_predictions, new_reaction_features, new_motion_sample = self.model.inference_step(
                    window_speaker,
                    self.past_reaction_features,
                    self.past_motion_sample
                )
            
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            
            # æ›´æ–°çŠ¶æ€
            self.past_reaction_features = new_reaction_features
            self.past_motion_sample = new_motion_sample
            
            # ä¿å­˜é¢„æµ‹ç»“æœï¼ˆåªä¿å­˜æœ€åä¸€å¸§æˆ–å…¨éƒ¨ï¼Œå–å†³äºéœ€æ±‚ï¼‰
            # è¿™é‡Œä¿å­˜æ•´ä¸ªçª—å£çš„é¢„æµ‹ï¼Œä½†æ ‡è®°ä¸ºå¯¹åº”çš„ç»“æŸå¸§
            frame_result = {
                'end_frame': end_frame,
                'start_frame': start_frame,
                'predictions': {k: v.cpu().numpy().squeeze(0) for k, v in current_predictions.items()},
                'inference_time': inference_time
            }
            
            self.frame_predictions.append(frame_result)
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'inf_time': f"{inference_time*1000:.1f}ms",
                'avg_time': f"{np.mean(self.inference_times[-10:])*1000:.1f}ms"
            })
        
        print(f"âœ… æ»‘åŠ¨çª—å£æ¨ç†å®Œæˆ")
        print(f"  å¤„ç†äº† {len(self.frame_predictions)} ä¸ªä½ç½®")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {np.mean(self.inference_times)*1000:.2f}ms")
        print(f"  æ€»æ¨ç†æ—¶é—´: {sum(self.inference_times):.2f}s")
        
        return self.frame_predictions
    
    def compare_stride_effects(self, sample_data, strides=[1, 4, 8, 16], max_frames=100):
        """æ¯”è¾ƒä¸åŒæ­¥é•¿çš„æ•ˆæœ"""
        print("ğŸ”„ æ¯”è¾ƒä¸åŒæ­¥é•¿çš„æ¨ç†æ•ˆæœ...")
        
        results = {}
        
        for stride in strides:
            print(f"\nğŸ“Š æµ‹è¯•æ­¥é•¿: {stride}")
            
            start_time = time.time()
            predictions = self.sliding_window_inference(sample_data, stride, max_frames)
            total_time = time.time() - start_time
            
            results[stride] = {
                'predictions': predictions,
                'total_time': total_time,
                'avg_inference_time': np.mean(self.inference_times),
                'num_inferences': len(predictions),
                'throughput': len(predictions) / total_time if total_time > 0 else 0
            }
            
            print(f"  æ¨ç†æ¬¡æ•°: {len(predictions)}")
            print(f"  æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  ååé‡: {results[stride]['throughput']:.1f} inferences/sec")
        
        # æ‰“å°å¯¹æ¯”è¡¨æ ¼
        print(f"\nğŸ“ˆ æ­¥é•¿å¯¹æ¯”æ€»ç»“:")
        print(f"{'æ­¥é•¿':<6} {'æ¨ç†æ¬¡æ•°':<8} {'æ€»æ—¶é—´(s)':<10} {'å¹³å‡æ—¶é—´(ms)':<12} {'ååé‡':<10}")
        print("-" * 60)
        
        for stride, data in results.items():
            print(f"{stride:<6} {data['num_inferences']:<8} {data['total_time']:<10.2f} "
                  f"{data['avg_inference_time']*1000:<12.1f} {data['throughput']:<10.1f}")
        
        return results
    
    def denormalize_features(self, data_dict):
        """åå½’ä¸€åŒ–ç‰¹å¾"""
        result = {}
        for key, value in data_dict.items():
            if key == 'landmarks':
                result[key] = value * 256.0
            elif key == 'au':
                result[key] = value * 5.0
            else:  # pose, gazeä¿æŒåŸå€¼
                result[key] = value
        return result
    
    def save_complete_results(self, predictions, sample_data, output_dir='sliding_results'):
        """ä¿å­˜å®Œæ•´çš„æ»‘åŠ¨çª—å£ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å®Œæ•´æ»‘åŠ¨çª—å£ç»“æœ...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # æå–æ‰€æœ‰ç‰¹å¾çš„é¢„æµ‹åºåˆ—
        pred_sequences = {
            'landmarks': [],
            'au': [],
            'pose': [],
            'gaze': []
        }
        
        frame_indices = []
        
        for pred in predictions:
            frame_indices.append(pred['end_frame'] - 1)  # è½¬æ¢ä¸º0-basedç´¢å¼•
            
            for feature_name in pred_sequences.keys():
                if feature_name in pred['predictions']:
                    feature_data = pred['predictions'][feature_name]
                    # å–æ¯ä¸ªçª—å£é¢„æµ‹çš„æœ€åä¸€å¸§
                    if len(feature_data.shape) > 1:
                        feature_data = feature_data[-1]
                    pred_sequences[feature_name].append(feature_data)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶åå½’ä¸€åŒ–
        pred_denorm = {}
        for feature_name, feature_list in pred_sequences.items():
            if feature_list:
                feature_array = np.array(feature_list)
                pred_denorm[feature_name] = feature_array
        
        pred_denorm = self.denormalize_features(pred_denorm)
        
        # è·å–å¯¹åº”çš„GTæ•°æ®
        gt_denorm = {}
        speaker_denorm = {}
        
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            if feature_name in sample_data['listener']:
                gt_data = sample_data['listener'][feature_name].numpy()
                gt_aligned = gt_data[frame_indices]  # åªå–æœ‰é¢„æµ‹çš„å¸§
                gt_denorm[feature_name] = gt_aligned
            
            if feature_name in sample_data['speaker']:
                speaker_data = sample_data['speaker'][feature_name].numpy()
                speaker_aligned = speaker_data[frame_indices]  # åªå–æœ‰é¢„æµ‹çš„å¸§
                speaker_denorm[feature_name] = speaker_aligned
        
        gt_denorm = self.denormalize_features(gt_denorm)
        speaker_denorm = self.denormalize_features(speaker_denorm)
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾ä¸ºCSV
        def save_all_features_csv(features_dict, prefix, output_dir):
            for feature_name, feature_data in features_dict.items():
                filename = os.path.join(output_dir, f'sliding_{prefix}_{feature_name}.csv')
                
                if feature_name == 'landmarks':
                    columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
                elif feature_name == 'au':
                    columns = [f'AU_{i}' for i in range(feature_data.shape[1])]
                elif feature_name == 'pose':
                    columns = ['pose_Rx', 'pose_Ry', 'pose_Rz']
                elif feature_name == 'gaze':
                    columns = ['gaze_angle_x', 'gaze_angle_y']
                
                df = pd.DataFrame(feature_data, columns=columns)
                df.insert(0, 'frame', frame_indices)
                df.to_csv(filename, index=False)
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾
        save_all_features_csv(speaker_denorm, 'speaker', output_dir)
        save_all_features_csv(gt_denorm, 'gt_listener', output_dir)
        save_all_features_csv(pred_denorm, 'predicted_listener', output_dir)
        
        print(f"ğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ° {output_dir}:")
        print(f"   Speakerç‰¹å¾: sliding_speaker_*.csv")
        print(f"   GT Listenerç‰¹å¾: sliding_gt_listener_*.csv")
        print(f"   é¢„æµ‹Listenerç‰¹å¾: sliding_predicted_listener_*.csv")
        
        return pred_denorm, gt_denorm, speaker_denorm, frame_indices
    
    def calculate_metrics(self, pred_denorm, gt_denorm):
        """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {}
        
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            if feature_name in pred_denorm and feature_name in gt_denorm:
                pred_data = pred_denorm[feature_name]
                gt_data = gt_denorm[feature_name]
                
                # ç¡®ä¿é•¿åº¦ä¸€è‡´
                min_len = min(len(pred_data), len(gt_data))
                pred_data = pred_data[:min_len]
                gt_data = gt_data[:min_len]
                
                # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
                mse = np.mean((pred_data - gt_data) ** 2)
                mae = np.mean(np.abs(pred_data - gt_data))
                rmse = np.sqrt(mse)
                
                metrics[feature_name] = {
                    'mse': mse,
                    'mae': mae,
                    'rmse': rmse
                }
                
                print(f"  {feature_name.upper()}:")
                print(f"    MSE: {mse:.6f}")
                print(f"    MAE: {mae:.6f}")
                print(f"    RMSE: {rmse:.6f}")
                
                # landmarksçš„è¯¦ç»†åˆ†æ
                if feature_name == 'landmarks':
                    x_errors = np.abs(pred_data[:, :68] - gt_data[:, :68])
                    y_errors = np.abs(pred_data[:, 68:] - gt_data[:, 68:])
                    
                    metrics[feature_name]['x_mae'] = np.mean(x_errors)
                    metrics[feature_name]['y_mae'] = np.mean(y_errors)
                    metrics[feature_name]['max_point_error'] = np.max(np.sqrt(x_errors**2 + y_errors**2))
                    
                    print(f"    Xåæ ‡MAE: {metrics[feature_name]['x_mae']:.6f}")
                    print(f"    Yåæ ‡MAE: {metrics[feature_name]['y_mae']:.6f}")
                    print(f"    æœ€å¤§ç‚¹è¯¯å·®: {metrics[feature_name]['max_point_error']:.6f}")
        
        return metrics
    
    def analyze_sliding_window_output(self, predictions, sample_data, output_dir='sliding_results'):
        """åˆ†ææ»‘åŠ¨çª—å£è¾“å‡ºï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("ğŸ“Š åˆ†ææ»‘åŠ¨çª—å£è¾“å‡º...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        pred_denorm, gt_denorm, speaker_denorm, frame_indices = self.save_complete_results(
            predictions, sample_data, output_dir
        )
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = self.calculate_metrics(pred_denorm, gt_denorm)
        
        # åˆ†æå¹³æ»‘æ€§ï¼ˆä½¿ç”¨landmarksï¼‰
        if 'landmarks' in pred_denorm:
            landmarks_sequence = pred_denorm['landmarks']
            
            if len(landmarks_sequence) > 1:
                frame_diffs = np.diff(landmarks_sequence, axis=0)
                smoothness = np.mean(np.abs(frame_diffs))
                
                print(f"\nğŸ¯ å¹³æ»‘æ€§åˆ†æ:")
                print(f"  è¾“å‡ºå¸§æ•°: {len(landmarks_sequence)}")
                print(f"  å¸§é—´å¹³æ»‘æ€§: {smoothness:.6f}")
                
                # ä¿å­˜å¹³æ»‘æ€§åˆ†æå›¾
                self.create_smoothness_plots(frame_diffs, output_dir)
                
                # åˆ›å»ºè°ƒè¯•é™æ€å›¾
                self.create_debug_static_plot(
                    speaker_denorm, gt_denorm, pred_denorm, frame_indices, output_dir
                )
                
                return {
                    'smoothness': smoothness,
                    'output_frames': len(landmarks_sequence),
                    'metrics': metrics,
                    'frame_indices': frame_indices
                }
        
        return {
            'smoothness': 0,
            'output_frames': 0,
            'metrics': metrics,
            'frame_indices': frame_indices
        }
    
    def create_smoothness_plots(self, frame_diffs, output_dir):
        """åˆ›å»ºå¹³æ»‘æ€§åˆ†æå›¾"""
        plt.figure(figsize=(15, 10))
        
        # å¸§é—´å·®å¼‚è¶‹åŠ¿
        plt.subplot(2, 3, 1)
        plt.plot(np.abs(frame_diffs).mean(axis=1))
        plt.title('å¸§é—´å·®å¼‚è¶‹åŠ¿')
        plt.xlabel('å¸§ç´¢å¼•')
        plt.ylabel('å¹³å‡ç»å¯¹å·®å¼‚')
        plt.grid(True, alpha=0.3)
        
        # å¸§é—´å·®å¼‚åˆ†å¸ƒ
        plt.subplot(2, 3, 2)
        plt.hist(np.abs(frame_diffs).flatten(), bins=50, alpha=0.7)
        plt.title('å¸§é—´å·®å¼‚åˆ†å¸ƒ')
        plt.xlabel('ç»å¯¹å·®å¼‚')
        plt.ylabel('é¢‘æ¬¡')
        plt.grid(True, alpha=0.3)
        
        # Xåæ ‡å·®å¼‚
        plt.subplot(2, 3, 3)
        x_diffs = frame_diffs[:, :68]
        plt.plot(np.abs(x_diffs).mean(axis=1), color='blue', alpha=0.7)
        plt.title('Xåæ ‡å¸§é—´å·®å¼‚')
        plt.xlabel('å¸§ç´¢å¼•')
        plt.ylabel('å¹³å‡ç»å¯¹å·®å¼‚')
        plt.grid(True, alpha=0.3)
        
        # Yåæ ‡å·®å¼‚
        plt.subplot(2, 3, 4)
        y_diffs = frame_diffs[:, 68:]
        plt.plot(np.abs(y_diffs).mean(axis=1), color='red', alpha=0.7)
        plt.title('Yåæ ‡å¸§é—´å·®å¼‚')
        plt.xlabel('å¸§ç´¢å¼•')
        plt.ylabel('å¹³å‡ç»å¯¹å·®å¼‚')
        plt.grid(True, alpha=0.3)
        
        # è¿åŠ¨é€Ÿåº¦åˆ†æ
        plt.subplot(2, 3, 5)
        motion_speed = np.sqrt(np.sum(frame_diffs**2, axis=1))
        plt.plot(motion_speed, color='purple', alpha=0.7)
        plt.title('è¿åŠ¨é€Ÿåº¦è¶‹åŠ¿')
        plt.xlabel('å¸§ç´¢å¼•')
        plt.ylabel('è¿åŠ¨é€Ÿåº¦')
        plt.grid(True, alpha=0.3)
        
        # å…³é”®ç‚¹è¿åŠ¨çƒ­å›¾
        plt.subplot(2, 3, 6)
        point_motion = np.sqrt(frame_diffs[:, :68]**2 + frame_diffs[:, 68:]**2)
        plt.imshow(point_motion.T, aspect='auto', cmap='hot', interpolation='nearest')
        plt.title('å…³é”®ç‚¹è¿åŠ¨çƒ­å›¾')
        plt.xlabel('å¸§ç´¢å¼•')
        plt.ylabel('å…³é”®ç‚¹ç´¢å¼•')
        plt.colorbar(label='è¿åŠ¨å¹…åº¦')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'smoothness_analysis.png'), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… å¹³æ»‘æ€§åˆ†æå›¾å·²ä¿å­˜")
    
    def create_debug_static_plot(self, speaker_denorm, gt_denorm, pred_denorm, frame_indices, output_dir):
        """åˆ›å»ºè°ƒè¯•é™æ€å›¾"""
        print("ğŸ” åˆ›å»ºè°ƒè¯•é™æ€å›¾...")
        
        # é€‰æ‹©ç¬¬ä¸€å¸§è¿›è¡Œåˆ†æ
        if len(frame_indices) == 0:
            return
            
        first_idx = 0
        
        speaker_x = speaker_denorm['landmarks'][first_idx, :68]
        speaker_y = speaker_denorm['landmarks'][first_idx, 68:]
        gt_x = gt_denorm['landmarks'][first_idx, :68]
        gt_y = gt_denorm['landmarks'][first_idx, 68:]
        pred_x = pred_denorm['landmarks'][first_idx, :68]
        pred_y = pred_denorm['landmarks'][first_idx, 68:]
        
        # åˆ›å»º2x2å­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # è®¡ç®—åæ ‡èŒƒå›´
        all_x = np.concatenate([speaker_x, gt_x, pred_x])
        all_y = np.concatenate([speaker_y, gt_y, pred_y])
        x_range = np.max(all_x) - np.min(all_x)
        y_range = np.max(all_y) - np.min(all_y)
        margin = 0.1 * max(x_range, y_range)
        
        # å·¦ä¸Šï¼šä¸‰è·¯å¯¹æ¯”
        ax1 = axes[0, 0]
        ax1.set_title(f'ç¬¬ä¸€å¸§å¯¹æ¯” - å¸§ {frame_indices[first_idx]}', fontsize=14, fontweight='bold')
        ax1.scatter(speaker_x, speaker_y, c='blue', s=30, alpha=0.7, label='Speaker')
        ax1.scatter(gt_x, gt_y, c='green', s=30, alpha=0.7, label='GT Listener')
        ax1.scatter(pred_x, pred_y, c='red', s=30, alpha=0.7, label='Predicted')
        
        self.draw_face_connections(ax1, speaker_x, speaker_y, 'blue')
        self.draw_face_connections(ax1, gt_x, gt_y, 'green')
        self.draw_face_connections(ax1, pred_x, pred_y, 'red')
        
        ax1.set_xlim(np.min(all_x) - margin, np.max(all_x) + margin)
        ax1.set_ylim(np.max(all_y) + margin, np.min(all_y) - margin)
        ax1.set_aspect('equal')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³ä¸Šï¼šè¯¯å·®åˆ†æ
        ax2 = axes[0, 1]
        ax2.set_title('é¢„æµ‹è¯¯å·®çƒ­å›¾', fontsize=14, fontweight='bold')
        
        errors = np.sqrt((gt_x - pred_x)**2 + (gt_y - pred_y)**2)
        scatter = ax2.scatter(gt_x, gt_y, c=errors, s=50, cmap='Reds', alpha=0.8)
        plt.colorbar(scatter, ax=ax2, label='é¢„æµ‹è¯¯å·®')
        
        ax2.set_xlim(gt_x.min() - 10, gt_x.max() + 10)
        ax2.set_ylim(gt_y.max() + 10, gt_y.min() - 10)
        ax2.set_aspect('equal')
        ax2.grid(True, alpha=0.3)
        
        # æ·»åŠ è¯¯å·®ç»Ÿè®¡
        error_stats = f"å¹³å‡è¯¯å·®: {errors.mean():.2f}\næœ€å¤§è¯¯å·®: {errors.max():.2f}\nè¯¯å·®æ ‡å‡†å·®: {errors.std():.2f}"
        ax2.text(0.02, 0.98, error_stats, transform=ax2.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        # å·¦ä¸‹ï¼šæ•°æ®ç»Ÿè®¡
        ax3 = axes[1, 0]
        ax3.axis('off')
        ax3.set_title('æ»‘åŠ¨çª—å£ç»Ÿè®¡', fontsize=14, fontweight='bold')
        
        stats_text = f"""
æ»‘åŠ¨çª—å£æ¨ç†ç»Ÿè®¡:

æ€»å¤„ç†å¸§æ•°: {len(frame_indices)}
å¹³å‡æ¨ç†æ—¶é—´: {np.mean(self.inference_times)*1000:.2f}ms
æœ€å¤§æ¨ç†æ—¶é—´: {np.max(self.inference_times)*1000:.2f}ms
æœ€å°æ¨ç†æ—¶é—´: {np.min(self.inference_times)*1000:.2f}ms

åæ ‡èŒƒå›´åˆ†æ:
Speaker: X[{speaker_x.min():.1f}, {speaker_x.max():.1f}], Y[{speaker_y.min():.1f}, {speaker_y.max():.1f}]
GT:      X[{gt_x.min():.1f}, {gt_x.max():.1f}], Y[{gt_y.min():.1f}, {gt_y.max():.1f}]
Pred:    X[{pred_x.min():.1f}, {pred_x.max():.1f}], Y[{pred_y.min():.1f}, {pred_y.max():.1f}]
        """
        
        ax3.text(0.05, 0.95, stats_text, transform=ax3.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace')
        
        # å³ä¸‹ï¼šæ¨ç†æ—¶é—´è¶‹åŠ¿
        ax4 = axes[1, 1]
        ax4.set_title('æ¨ç†æ—¶é—´è¶‹åŠ¿', fontsize=14, fontweight='bold')
        
        times_ms = np.array(self.inference_times) * 1000
        ax4.plot(times_ms, 'b-', alpha=0.7)
        ax4.axhline(y=np.mean(times_ms), color='r', linestyle='--', 
                   label=f'å¹³å‡: {np.mean(times_ms):.1f}ms')
        ax4.set_xlabel('çª—å£ç´¢å¼•')
        ax4.set_ylabel('æ¨ç†æ—¶é—´ (æ¯«ç§’)')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        debug_path = os.path.join(output_dir, 'sliding_debug_first_frame.png')
        plt.savefig(debug_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è°ƒè¯•é™æ€å›¾å·²ä¿å­˜åˆ°: {debug_path}")
    
    def create_comparison_animation(self, predictions, sample_data, output_path='sliding_results/sliding_comparison_300.gif'):
        """åˆ›å»ºä¸‰è·¯å¯¹æ¯”åŠ¨ç”»ï¼ˆSpeaker + GT + Predictedï¼‰"""
        print("ğŸ¬ åˆ›å»ºæ»‘åŠ¨çª—å£ä¸‰è·¯å¯¹æ¯”åŠ¨ç”»...")
        
        # æå–é¢„æµ‹landmarksåºåˆ—
        pred_landmarks = []
        frame_indices = []
        
        for pred in predictions:
            landmarks = pred['predictions']['landmarks']
            if len(landmarks.shape) > 1:
                landmarks = landmarks[-1]  # å–æœ€åä¸€å¸§
            pred_landmarks.append(landmarks * 256.0)  # åå½’ä¸€åŒ–
            frame_indices.append(pred['end_frame'] - 1)  # è½¬æ¢ä¸º0-basedç´¢å¼•
        
        pred_landmarks = np.array(pred_landmarks)
        
        # è·å–å¯¹åº”çš„çœŸå®landmarkså’Œspeaker landmarks
        gt_landmarks = sample_data['listener']['landmarks'].numpy() * 256.0
        speaker_landmarks = sample_data['speaker']['landmarks'].numpy() * 256.0
        
        # åªä¿ç•™æœ‰é¢„æµ‹çš„å¸§
        gt_landmarks_aligned = gt_landmarks[frame_indices]
        speaker_landmarks_aligned = speaker_landmarks[frame_indices]
        
        n_frames = min(len(pred_landmarks), 600)  # é™åˆ¶åŠ¨ç”»é•¿åº¦
        
        # æå–åæ ‡
        pred_x = pred_landmarks[:n_frames, :68]
        pred_y = pred_landmarks[:n_frames, 68:]
        gt_x = gt_landmarks_aligned[:n_frames, :68]
        gt_y = gt_landmarks_aligned[:n_frames, 68:]
        speaker_x = speaker_landmarks_aligned[:n_frames, :68]
        speaker_y = speaker_landmarks_aligned[:n_frames, 68:]
        
        # è®¡ç®—åæ ‡èŒƒå›´
        all_x = np.concatenate([pred_x.flatten(), gt_x.flatten(), speaker_x.flatten()])
        all_y = np.concatenate([pred_y.flatten(), gt_y.flatten(), speaker_y.flatten()])
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        margin = 0.1 * max(x_max - x_min, y_max - y_min)
        
        # åˆ›å»ºä¸‰åˆ—å­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        def animate(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in [ax1, ax2, ax3]:
                ax.clear()
                ax.set_xlim(x_min - margin, x_max + margin)
                ax.set_ylim(y_max + margin, y_min - margin)
                ax.set_aspect('equal')
                ax.grid(True, alpha=0.3)
            
            # è®¾ç½®æ ‡é¢˜
            ax1.set_title(f'Speaker - å¸§ {frame_indices[frame]}', fontsize=12)
            ax2.set_title(f'GT Listener - å¸§ {frame_indices[frame]}', fontsize=12)
            ax3.set_title(f'æ»‘åŠ¨çª—å£é¢„æµ‹ - å¸§ {frame_indices[frame]}', fontsize=12)
            
            # ç»˜åˆ¶ç‰¹å¾ç‚¹
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7)
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7)
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7)
            
            # ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿
            self.draw_face_connections(ax1, speaker_x[frame], speaker_y[frame], 'blue')
            self.draw_face_connections(ax2, gt_x[frame], gt_y[frame], 'green')
            self.draw_face_connections(ax3, pred_x[frame], pred_y[frame], 'red')
        
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, interval=200, repeat=True)
        anim.save(output_path, writer='pillow', fps=25, dpi=80)
        plt.close()
        
        print(f"âœ… æ»‘åŠ¨çª—å£ä¸‰è·¯å¯¹æ¯”åŠ¨ç”»å·²ä¿å­˜åˆ°: {output_path}")
        return output_path
    
    def create_performance_analysis(self, output_dir):
        """åˆ›å»ºæ€§èƒ½åˆ†æå›¾è¡¨"""
        print("ğŸ“ˆ åˆ›å»ºæ€§èƒ½åˆ†æå›¾è¡¨...")
        
        if not self.inference_times:
            print("âš ï¸ æ²¡æœ‰æ¨ç†æ—¶é—´æ•°æ®")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ¨ç†æ—¶é—´è¶‹åŠ¿
        ax1 = axes[0, 0]
        times_ms = np.array(self.inference_times) * 1000
        ax1.plot(times_ms, 'b-', alpha=0.7)
        ax1.axhline(y=np.mean(times_ms), color='r', linestyle='--', 
                   label=f'å¹³å‡å€¼: {np.mean(times_ms):.2f}ms')
        ax1.set_title('æ»‘åŠ¨çª—å£æ¨ç†æ—¶é—´è¶‹åŠ¿')
        ax1.set_xlabel('çª—å£ç´¢å¼•')
        ax1.set_ylabel('æ¨ç†æ—¶é—´ (æ¯«ç§’)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ¨ç†æ—¶é—´åˆ†å¸ƒ
        ax2 = axes[0, 1]
        ax2.hist(times_ms, bins=20, alpha=0.7, edgecolor='black')
        ax2.axvline(x=np.mean(times_ms), color='r', linestyle='--',
                   label=f'å¹³å‡å€¼: {np.mean(times_ms):.2f}ms')
        ax2.set_title('æ¨ç†æ—¶é—´åˆ†å¸ƒ')
        ax2.set_xlabel('æ¨ç†æ—¶é—´ (æ¯«ç§’)')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # ç´¯ç§¯æ¨ç†æ—¶é—´
        ax3 = axes[1, 0]
        cumulative_time = np.cumsum(self.inference_times)
        ax3.plot(cumulative_time, 'g-', alpha=0.7)
        ax3.set_title('ç´¯ç§¯æ¨ç†æ—¶é—´')
        ax3.set_xlabel('çª—å£ç´¢å¼•')
        ax3.set_ylabel('ç´¯ç§¯æ—¶é—´ (ç§’)')
        ax3.grid(True, alpha=0.3)
        
        # æ€§èƒ½ç»Ÿè®¡è¡¨
        ax4 = axes[1, 1]
        ax4.axis('off')
        ax4.set_title('æ€§èƒ½ç»Ÿè®¡æ€»ç»“', fontsize=14, fontweight='bold')
        
        stats_text = f"""
æ»‘åŠ¨çª—å£æ¨ç†æ€§èƒ½ç»Ÿè®¡:

æ€»çª—å£æ•°: {len(self.inference_times)}
æ€»æ¨ç†æ—¶é—´: {sum(self.inference_times):.2f}s
å¹³å‡æ¨ç†æ—¶é—´: {np.mean(self.inference_times)*1000:.2f}ms
æ ‡å‡†å·®: {np.std(self.inference_times)*1000:.2f}ms
æœ€å°æ¨ç†æ—¶é—´: {np.min(self.inference_times)*1000:.2f}ms
æœ€å¤§æ¨ç†æ—¶é—´: {np.max(self.inference_times)*1000:.2f}ms

ååé‡: {len(self.inference_times)/sum(self.inference_times):.1f} windows/sec
ç†è®ºå»¶è¿Ÿ: {self.window_size/25*1000:.0f}ms ({self.window_size}å¸§@25fps)
        """
        
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=11,
                verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        performance_path = os.path.join(output_dir, 'sliding_performance_analysis.png')
        plt.savefig(performance_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {performance_path}")
        return performance_path
    
    def draw_face_connections(self, ax, x_coords, y_coords, color='blue'):
        """ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿"""
        connections = {
            'jaw': list(range(0, 17)),
            'right_eyebrow': list(range(17, 22)),
            'left_eyebrow': list(range(22, 27)),
            'nose_bridge': list(range(27, 31)),
            'nose_tip': list(range(31, 36)),
            'right_eye': list(range(36, 42)) + [36],
            'left_eye': list(range(42, 48)) + [42],
            'outer_lip': list(range(48, 60)) + [48],
            'inner_lip': list(range(60, 68)) + [60]
        }
        
        for name, indices in connections.items():
            if len(indices) > 1:
                try:
                    x_line = [x_coords[idx] for idx in indices]
                    y_line = [y_coords[idx] for idx in indices]
                    ax.plot(x_line, y_line, color=color, linewidth=1, alpha=0.6)
                except IndexError:
                    continue


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é‡å çª—å£æ¨ç†æµ‹è¯•")
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data-csv', type=str,
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv',
                       help='æµ‹è¯•æ•°æ®CSVè·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='æµ‹è¯•æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--stride', type=int, default=1,
                       help='æ»‘åŠ¨æ­¥é•¿ (1=é€å¸§, 4=æ¯4å¸§, 16=éé‡å )')
    parser.add_argument('--max-frames', type=int, default=700,
                       help='æœ€å¤§å¤„ç†å¸§æ•°')
    parser.add_argument('--compare-strides', action='store_true',
                       help='æ¯”è¾ƒä¸åŒæ­¥é•¿çš„æ•ˆæœ')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not os.path.exists(args.data_csv):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_csv}")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SlidingWindowTester(args.checkpoint, args.device)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
        dataset = SpeakerListenerDataset(args.data_csv)
        sample = dataset[args.sample_idx]
        
        if sample is None:
            print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ ·æœ¬")
            return
        
        print(f"âœ… æµ‹è¯•æ ·æœ¬åŠ è½½æˆåŠŸ (æ€»å¸§æ•°: {sample['speaker']['landmarks'].shape[0]})")
        
        if args.compare_strides:
            # æ¯”è¾ƒä¸åŒæ­¥é•¿
            results = tester.compare_stride_effects(sample)
            
            print(f"\nğŸ¯ ä¸åŒæ­¥é•¿å¯¹æ¯”å®Œæˆï¼")
            
        else:
            # å•ä¸€æ­¥é•¿æµ‹è¯•
            predictions = tester.sliding_window_inference(sample, args.stride, args.max_frames)
            
            # å®Œæ•´åˆ†æç»“æœ
            analysis = tester.analyze_sliding_window_output(predictions, sample)
            
            # åˆ›å»ºæ€§èƒ½åˆ†æ
            performance_path = tester.create_performance_analysis('sliding_results')
            
            # åˆ›å»ºä¸‰è·¯å¯¹æ¯”åŠ¨ç”»
            animation_path = tester.create_comparison_animation(predictions, sample)
            
            print(f"\nğŸ¯ æ»‘åŠ¨çª—å£æµ‹è¯•ç»“æœæ€»ç»“:")
            print("="*60)
            print(f"ğŸš€ æµ‹è¯•é…ç½®:")
            print(f"  æ»‘åŠ¨æ­¥é•¿: {args.stride}")
            print(f"  çª—å£å¤§å°: {tester.window_size}")
            print(f"  å¤„ç†å¸§æ•°: {args.max_frames}")
            
            print(f"\nğŸ“Š è¾“å‡ºç»Ÿè®¡:")
            print(f"  è¾“å‡ºå¸§æ•°: {analysis['output_frames']}")
            print(f"  å¸§é—´å¹³æ»‘æ€§: {analysis['smoothness']:.6f}")
            print(f"  å¤„ç†çª—å£æ•°: {len(predictions)}")
            
            print(f"\nâš¡ æ€§èƒ½ç»Ÿè®¡:")
            if tester.inference_times:
                print(f"  å¹³å‡æ¨ç†æ—¶é—´: {np.mean(tester.inference_times)*1000:.2f}ms")
                print(f"  æœ€å¤§æ¨ç†æ—¶é—´: {np.max(tester.inference_times)*1000:.2f}ms")
                print(f"  æœ€å°æ¨ç†æ—¶é—´: {np.min(tester.inference_times)*1000:.2f}ms")
                print(f"  æ€»æ¨ç†æ—¶é—´: {sum(tester.inference_times):.2f}s")
                print(f"  æ¨ç†ååé‡: {len(tester.inference_times)/sum(tester.inference_times):.1f} windows/sec")
            
            print(f"\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:")
            if 'metrics' in analysis:
                for feature_name, metrics in analysis['metrics'].items():
                    print(f"  {feature_name.upper()}:")
                    print(f"    MAE: {metrics['mae']:.6f}")
                    print(f"    RMSE: {metrics['rmse']:.6f}")
            
            print(f"\nğŸ“ ç”Ÿæˆæ–‡ä»¶:")
            print(f"  ç»“æœç›®å½•: sliding_results/")
            print(f"  ä¸‰è·¯å¯¹æ¯”åŠ¨ç”»: {animation_path}")
            print(f"  æ€§èƒ½åˆ†æå›¾: {performance_path}")
            print(f"  è°ƒè¯•é™æ€å›¾: sliding_results/sliding_debug_first_frame.png")
            print(f"  å¹³æ»‘æ€§åˆ†æ: sliding_results/smoothness_analysis.png")
            print(f"  CSVç»“æœæ–‡ä»¶: sliding_results/sliding_*_*.csv")
            
            print("="*60)
            
            # ä¿å­˜æµ‹è¯•æ€»ç»“
            summary = {
                'config': {
                    'stride': args.stride,
                    'window_size': tester.window_size,
                    'max_frames': args.max_frames,
                    'sample_idx': args.sample_idx
                },
                'performance': {
                    'avg_inference_time_ms': float(np.mean(tester.inference_times) * 1000) if tester.inference_times else 0,
                    'total_inference_time_s': float(sum(tester.inference_times)) if tester.inference_times else 0,
                    'throughput_windows_per_sec': float(len(tester.inference_times)/sum(tester.inference_times)) if tester.inference_times and sum(tester.inference_times) > 0 else 0
                },
                'results': {
                    'output_frames': analysis['output_frames'],
                    'smoothness': float(analysis['smoothness']),
                    'num_windows_processed': len(predictions)
                },
                'metrics': {k: {mk: float(mv) for mk, mv in v.items()} for k, v in analysis.get('metrics', {}).items()},
                'files': {
                    'animation': animation_path,
                    'performance_plot': performance_path,
                    'debug_plot': 'sliding_results/sliding_debug_first_frame.png',
                    'smoothness_plot': 'sliding_results/smoothness_analysis.png'
                }
            }
            
            # ä¿å­˜æµ‹è¯•æ€»ç»“JSON
            import json
            summary_path = 'sliding_results/test_summary.json'
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2)
            
            print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“å·²ä¿å­˜: {summary_path}")
            
            return summary
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()