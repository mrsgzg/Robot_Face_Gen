#!/usr/bin/env python3
"""
ç®€æ˜“æ¨¡å‹æµ‹è¯•è„šæœ¬
åŠ è½½checkpointï¼Œç”Ÿæˆé¢„æµ‹landmarksï¼Œä¿å­˜ç»“æœå¹¶å¯è§†åŒ–
"""

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from torch.utils.data import DataLoader
import argparse
from tqdm import tqdm

# å¯¼å…¥ä½ çš„æ¨¡å‹å’Œæ•°æ®é›†
from facial_reaction_model.model import FacialReactionModel
from Data_Set import SpeakerListenerDataset


class SimpleTester:
    """ç®€æ˜“æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.checkpoint_path = checkpoint_path
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {self.checkpoint_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # ä»checkpointè·å–æ¨¡å‹é…ç½®
        config = checkpoint.get('config', {})
        model_config = config.get('model', {})
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = FacialReactionModel(
            feature_dim=model_config.get('feature_dim', 256),
            audio_dim=model_config.get('audio_dim', 384),
            period=model_config.get('period', 25),
            max_seq_len=model_config.get('max_seq_len', 750),
            device=str(self.device),
            window_size=model_config.get('window_size', 16),
            momentum=model_config.get('momentum', 0.9)
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œepoch: {checkpoint.get('epoch', 'unknown')}")
        
    def load_test_sample(self, data_csv, sample_idx=0):
        """åŠ è½½å•ä¸ªæµ‹è¯•æ ·æœ¬"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {data_csv}, æ ·æœ¬ç´¢å¼•: {sample_idx}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SpeakerListenerDataset(data_csv)
        
        # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
        if sample_idx >= len(dataset):
            sample_idx = 0
            print(f"âš ï¸ æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨ç´¢å¼• 0")
        
        # è·å–æ ·æœ¬
        sample = dataset[sample_idx]
        if sample is None:
            raise ValueError(f"æ— æ³•åŠ è½½æ ·æœ¬ {sample_idx}")
        
        # è½¬æ¢ä¸ºbatchæ ¼å¼å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        batch_data = {}
        for role in ['speaker', 'listener']:
            batch_data[role] = {}
            for key, value in sample[role].items():
                batch_data[role][key] = value.unsqueeze(0).to(self.device)
        
        print(f"âœ… æµ‹è¯•æ ·æœ¬åŠ è½½æˆåŠŸ")
        print(f"   Speaker landmarks: {batch_data['speaker']['landmarks'].shape}")
        print(f"   Speaker AU: {batch_data['speaker']['au'].shape}")
        print(f"   Listener landmarks: {batch_data['listener']['landmarks'].shape}")
        print(f"   Listener AU: {batch_data['listener']['au'].shape}")
        
        return batch_data
    
    def generate_predictions(self, batch_data, num_samples=3):
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆé¢„æµ‹ (é‡‡æ ·æ¬¡æ•°: {num_samples})")
        
        speaker_data = batch_data['speaker']
        
        predictions_list = []
        
        with torch.no_grad():
            for i in range(num_samples):
                predictions, distributions, _ = self.model(speaker_data, speaker_out=False)
                
                # éªŒè¯é¢„æµ‹ç»“æœåŒ…å«æ‰€æœ‰å¿…è¦çš„ç‰¹å¾
                expected_features = ['landmarks', 'au', 'pose', 'gaze']
                for feature in expected_features:
                    if feature not in predictions:
                        raise ValueError(f"é¢„æµ‹ç»“æœç¼ºå°‘ç‰¹å¾: {feature}")
                
                predictions_list.append(predictions)
        
        print(f"âœ… é¢„æµ‹ç”Ÿæˆå®Œæˆ")
        
        # è¿”å›ç¬¬ä¸€æ¬¡é¢„æµ‹ä½œä¸ºä¸»è¦ç»“æœï¼Œä»¥åŠæ‰€æœ‰é¢„æµ‹ç”¨äºå¤šæ ·æ€§åˆ†æ
        return predictions_list[0], predictions_list
    
    def save_results(self, speaker_data, gt_listener, predictions, output_dir='test_results'):
        """ä¿å­˜é¢„æµ‹ç»“æœä¸ºCSV"""
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpyå¹¶å»æ‰batchç»´åº¦
        def to_numpy(tensor_dict):
            return {k: v.cpu().numpy().squeeze(0) for k, v in tensor_dict.items()}
        
        speaker_np = to_numpy(speaker_data)
        gt_listener_np = to_numpy(gt_listener)
        pred_listener_np = to_numpy(predictions)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹æ•°æ®èŒƒå›´
        print("ğŸ” è°ƒè¯•ä¿¡æ¯ - æ•°æ®èŒƒå›´æ£€æŸ¥:")
        for name, data_dict in [("Speaker", speaker_np), ("GT Listener", gt_listener_np), ("Predicted", pred_listener_np)]:
            landmarks = data_dict['landmarks']
            print(f"  {name} landmarkså½’ä¸€åŒ–å‰èŒƒå›´: [{landmarks.min():.4f}, {landmarks.max():.4f}]")
            print(f"  {name} landmarkså½¢çŠ¶: {landmarks.shape}")
        
        # åå½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾ (æ ¹æ®Data_Set.pyä¸­çš„å½’ä¸€åŒ–æ–¹å¼)
        def denormalize_features(data_dict):
            result = data_dict.copy()
            result['landmarks'] = data_dict['landmarks'] * 256.0  # landmarksé™¤ä»¥256å½’ä¸€åŒ–
            result['au'] = data_dict['au'] * 5.0                  # AUé™¤ä»¥5å½’ä¸€åŒ–
            # poseå’Œgazeä¿æŒåŸå§‹å€¼ï¼Œæ— éœ€åå½’ä¸€åŒ–
            return result
        
        speaker_denorm = denormalize_features(speaker_np)
        gt_denorm = denormalize_features(gt_listener_np)
        pred_denorm = denormalize_features(pred_listener_np)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥åå½’ä¸€åŒ–åçš„æ•°æ®èŒƒå›´
        print("ğŸ” åå½’ä¸€åŒ–åæ•°æ®èŒƒå›´:")
        for name, data_dict in [("Speaker", speaker_denorm), ("GT Listener", gt_denorm), ("Predicted", pred_denorm)]:
            landmarks = data_dict['landmarks']
            print(f"  {name} landmarksèŒƒå›´: [{landmarks.min():.2f}, {landmarks.max():.2f}]")
            if landmarks.shape[1] == 136:
                x_coords = landmarks[:, :68]
                y_coords = landmarks[:, 68:]
                print(f"    Xåæ ‡èŒƒå›´: [{x_coords.min():.2f}, {x_coords.max():.2f}]")
                print(f"    Yåæ ‡èŒƒå›´: [{y_coords.min():.2f}, {y_coords.max():.2f}]")
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾ä¸ºCSV
        def save_all_features_csv(features_dict, prefix, output_dir):
            """ä¿å­˜æ‰€æœ‰ç‰¹å¾åˆ°CSVæ–‡ä»¶"""
            for feature_name, feature_data in features_dict.items():
                if feature_name == 'audio':  # éŸ³é¢‘ç‰¹å¾å¤ªå¤§ï¼Œè·³è¿‡
                    continue
                    
                filename = os.path.join(output_dir, f'{prefix}_{feature_name}.csv')
                
                if feature_name == 'landmarks':
                    # landmarksç‰¹æ®Šå¤„ç†ï¼šåˆ†ç¦»x,yåæ ‡
                    x_coords = feature_data[:, :68]
                    y_coords = feature_data[:, 68:]
                    columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
                    df = pd.DataFrame(feature_data, columns=columns)
                else:
                    # å…¶ä»–ç‰¹å¾ç›´æ¥ä¿å­˜
                    if feature_name == 'au':
                        columns = [f'AU_{i}' for i in range(feature_data.shape[1])]
                    elif feature_name == 'pose':
                        columns = ['pose_Rx', 'pose_Ry', 'pose_Rz']
                    elif feature_name == 'gaze':
                        columns = ['gaze_angle_x', 'gaze_angle_y']
                    else:
                        columns = [f'{feature_name}_{i}' for i in range(feature_data.shape[1])]
                    
                    df = pd.DataFrame(feature_data, columns=columns)
                
                df.insert(0, 'frame', range(len(df)))
                df.to_csv(filename, index=False)
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾
        save_all_features_csv(speaker_denorm, 'speaker', output_dir)
        save_all_features_csv(gt_denorm, 'gt_listener', output_dir)
        save_all_features_csv(pred_denorm, 'predicted_listener', output_dir)
        
        # ä¸ºå‘åå…¼å®¹ï¼Œä»ç„¶è¿”å›landmarksçš„DataFrame
        def load_landmarks_df(features_dict):
            landmarks = features_dict['landmarks']
            x_coords = landmarks[:, :68]
            y_coords = landmarks[:, 68:]
            columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
            df = pd.DataFrame(landmarks, columns=columns)
            df.insert(0, 'frame', range(len(df)))
            return df
        
        speaker_df = load_landmarks_df(speaker_denorm)
        gt_df = load_landmarks_df(gt_denorm)
        pred_df = load_landmarks_df(pred_denorm)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {output_dir}:")
        print(f"   Speakerç‰¹å¾: speaker_landmarks.csv, speaker_au.csv, speaker_pose.csv, speaker_gaze.csv")
        print(f"   GT Listenerç‰¹å¾: gt_listener_landmarks.csv, gt_listener_au.csv, gt_listener_pose.csv, gt_listener_gaze.csv")
        print(f"   é¢„æµ‹Listenerç‰¹å¾: predicted_listener_landmarks.csv, predicted_listener_au.csv, predicted_listener_pose.csv, predicted_listener_gaze.csv")
        
        return speaker_df, gt_df, pred_df
    
    def create_comparison_animation(self, speaker_df, gt_df, pred_df, 
                                  output_path='comparison_animation.gif', max_frames=300):
        """åˆ›å»ºä¸‰è·¯å¯¹æ¯”åŠ¨ç”»"""
        print(f"ğŸ¬ åˆ›å»ºå¯¹æ¯”åŠ¨ç”»: {output_path}")
        
        # é™åˆ¶å¸§æ•°
        n_frames = min(len(speaker_df), len(gt_df), len(pred_df), max_frames)
        
        # æå–åæ ‡æ•°æ®
        def extract_coords(df):
            x_cols = [f'x_{i}' for i in range(68)]
            y_cols = [f'y_{i}' for i in range(68)]
            x_coords = df[x_cols].values[:n_frames]
            y_coords = df[y_cols].values[:n_frames]
            return x_coords, y_coords
        
        speaker_x, speaker_y = extract_coords(speaker_df)
        gt_x, gt_y = extract_coords(gt_df)
        pred_x, pred_y = extract_coords(pred_df)
        
        # è°ƒè¯•ï¼šæ£€æŸ¥æå–çš„åæ ‡èŒƒå›´
        print("ğŸ” åæ ‡æ•°æ®åˆ†æ:")
        for name, x_data, y_data in [("Speaker", speaker_x, speaker_y), 
                                   ("GT", gt_x, gt_y), 
                                   ("Predicted", pred_x, pred_y)]:
            print(f"  {name}: XèŒƒå›´[{x_data.min():.2f}, {x_data.max():.2f}], YèŒƒå›´[{y_data.min():.2f}, {y_data.max():.2f}]")
            print(f"    ä¸­å¿ƒç‚¹: X={x_data.mean():.2f}, Y={y_data.mean():.2f}")
            print(f"    æ ‡å‡†å·®: X={x_data.std():.2f}, Y={y_data.std():.2f}")
        
        # æ™ºèƒ½åæ ‡èŒƒå›´è®¡ç®—
        all_x = np.concatenate([speaker_x.flatten(), gt_x.flatten(), pred_x.flatten()])
        all_y = np.concatenate([speaker_y.flatten(), gt_y.flatten(), pred_y.flatten()])
        
        # æ£€æŸ¥æ•°æ®æ˜¯å¦å¼‚å¸¸ï¼ˆæ¯”å¦‚éƒ½åœ¨ä¸€ä¸ªå¾ˆå°çš„èŒƒå›´å†…ï¼‰
        x_range = np.max(all_x) - np.min(all_x)
        y_range = np.max(all_y) - np.min(all_y)
        
        print(f"  æ€»ä½“èŒƒå›´: X={x_range:.2f}, Y={y_range:.2f}")
        
        # å¦‚æœèŒƒå›´å¤ªå°ï¼Œè¯´æ˜å¯èƒ½æœ‰é—®é¢˜
        if x_range < 10 or y_range < 10:
            print("âš ï¸ è­¦å‘Šï¼šåæ ‡èŒƒå›´å¼‚å¸¸å°ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®é—®é¢˜ï¼")
            print("å°è¯•ä½¿ç”¨ç›¸å¯¹åæ ‡å¯è§†åŒ–...")
            return self.create_relative_comparison_animation(speaker_df, gt_df, pred_df, output_path, max_frames)
        
        # è®¾ç½®åæ ‡è½´èŒƒå›´
        margin = 0.1 * max(x_range, y_range)
        x_min, x_max = np.min(all_x) - margin, np.max(all_x) + margin
        y_min, y_max = np.min(all_y) - margin, np.max(all_y) + margin
        
        # åˆ›å»ºä¸‰åˆ—å­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        def animate(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in [ax1, ax2, ax3]:
                ax.clear()
                ax.set_xlim(x_min, x_max)
                ax.set_ylim(y_max, y_min)  # ç¿»è½¬yè½´ï¼ˆå›¾åƒåæ ‡ç³»ï¼‰
                ax.set_aspect('equal')
                ax.grid(True, alpha=0.3)
            
            # è®¾ç½®æ ‡é¢˜
            ax1.set_title(f'Speaker - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax2.set_title(f'GT Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax3.set_title(f'Predicted Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            
            # ç»˜åˆ¶ç‰¹å¾ç‚¹
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7, label='Speaker')
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7, label='GT')
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7, label='Predicted')
            
            # ç»˜åˆ¶åŸºæœ¬è¿æ¥çº¿
            for ax, x_coords, y_coords, color in [(ax1, speaker_x[frame], speaker_y[frame], 'blue'),
                                                 (ax2, gt_x[frame], gt_y[frame], 'green'),
                                                 (ax3, pred_x[frame], pred_y[frame], 'red')]:
                self.draw_face_connections(ax, x_coords, y_coords, color)
        
        # åˆ›å»ºåŠ¨ç”»
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, 
                                     interval=100, repeat=True)
        
        # ä¿å­˜
        anim.save(output_path, writer='pillow', fps=10, dpi=80)
        plt.close()
        
        print(f"âœ… åŠ¨ç”»å·²ä¿å­˜åˆ°: {output_path}")
    
    def create_relative_comparison_animation(self, speaker_df, gt_df, pred_df, 
                                           output_path='relative_comparison_animation.gif', max_frames=300):
        """åˆ›å»ºç›¸å¯¹åæ ‡å¯¹æ¯”åŠ¨ç”»ï¼ˆå½“ç»å¯¹åæ ‡å¼‚å¸¸æ—¶ä½¿ç”¨ï¼‰"""
        print("ğŸ¬ åˆ›å»ºç›¸å¯¹åæ ‡å¯¹æ¯”åŠ¨ç”»...")
        
        n_frames = min(len(speaker_df), len(gt_df), len(pred_df), max_frames)
        
        # æå–åæ ‡æ•°æ®å¹¶è½¬æ¢ä¸ºç›¸å¯¹åæ ‡
        def extract_and_center_coords(df):
            x_cols = [f'x_{i}' for i in range(68)]
            y_cols = [f'y_{i}' for i in range(68)]
            x_coords = df[x_cols].values[:n_frames]
            y_coords = df[y_cols].values[:n_frames]
            
            # è½¬æ¢ä¸ºç›¸å¯¹åæ ‡ï¼ˆä»¥æ¯å¸§çš„è´¨å¿ƒä¸ºä¸­å¿ƒï¼‰
            rel_x_coords = []
            rel_y_coords = []
            for frame in range(n_frames):
                center_x = np.mean(x_coords[frame])
                center_y = np.mean(y_coords[frame])
                rel_x = x_coords[frame] - center_x
                rel_y = y_coords[frame] - center_y
                rel_x_coords.append(rel_x)
                rel_y_coords.append(rel_y)
            
            return np.array(rel_x_coords), np.array(rel_y_coords)
        
        speaker_x, speaker_y = extract_and_center_coords(speaker_df)
        gt_x, gt_y = extract_and_center_coords(gt_df)
        pred_x, pred_y = extract_and_center_coords(pred_df)
        
        # è®¡ç®—ç›¸å¯¹åæ ‡èŒƒå›´
        all_rel_x = np.concatenate([speaker_x.flatten(), gt_x.flatten(), pred_x.flatten()])
        all_rel_y = np.concatenate([speaker_y.flatten(), gt_y.flatten(), pred_y.flatten()])
        max_range = max(np.max(np.abs(all_rel_x)), np.max(np.abs(all_rel_y)))
        margin = 0.1 * max_range
        
        print(f"ç›¸å¯¹åæ ‡èŒƒå›´: Â±{max_range:.2f}")
        
        # åˆ›å»ºä¸‰åˆ—å­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        def animate(frame):
            for ax in [ax1, ax2, ax3]:
                ax.clear()
                ax.set_xlim(-max_range - margin, max_range + margin)
                ax.set_ylim(max_range + margin, -max_range - margin)
                ax.set_aspect('equal')
                ax.grid(True, alpha=0.3)
                ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)
                ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)
            
            ax1.set_title(f'Speaker (Relative) - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax2.set_title(f'GT Listener (Relative) - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax3.set_title(f'Predicted Listener (Relative) - å¸§ {frame+1}/{n_frames}', fontsize=12)
            
            # ç»˜åˆ¶ç‰¹å¾ç‚¹å’Œä¸­å¿ƒç‚¹
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7)
            ax1.scatter(0, 0, c='darkblue', s=50, marker='+', linewidth=2)
            
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7)
            ax2.scatter(0, 0, c='darkgreen', s=50, marker='+', linewidth=2)
            
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7)
            ax3.scatter(0, 0, c='darkred', s=50, marker='+', linewidth=2)
            
            # ç»˜åˆ¶è¿æ¥çº¿
            for ax, x_coords, y_coords, color in [(ax1, speaker_x[frame], speaker_y[frame], 'blue'),
                                                 (ax2, gt_x[frame], gt_y[frame], 'green'),
                                                 (ax3, pred_x[frame], pred_y[frame], 'red')]:
                self.draw_face_connections(ax, x_coords, y_coords, color)
        
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, interval=100, repeat=True)
        anim.save(output_path, writer='pillow', fps=10, dpi=80)
        plt.close()
        
        print(f"âœ… ç›¸å¯¹åæ ‡åŠ¨ç”»å·²ä¿å­˜åˆ°: {output_path}")
    
    def create_debug_static_plot(self, speaker_df, gt_df, pred_df, output_path='debug_first_frame.png'):
        """åˆ›å»ºç¬¬ä¸€å¸§çš„é™æ€è°ƒè¯•å›¾"""
        print("ğŸ” åˆ›å»ºè°ƒè¯•é™æ€å›¾...")
        
        # æå–ç¬¬ä¸€å¸§æ•°æ®
        def extract_first_frame(df):
            x_cols = [f'x_{i}' for i in range(68)]
            y_cols = [f'y_{i}' for i in range(68)]
            x_coords = df[x_cols].iloc[0].values
            y_coords = df[y_cols].iloc[0].values
            return x_coords, y_coords
        
        speaker_x, speaker_y = extract_first_frame(speaker_df)
        gt_x, gt_y = extract_first_frame(gt_df)
        pred_x, pred_y = extract_first_frame(pred_df)
        
        # åˆ›å»º2x2å­å›¾ï¼šå·¦ä¸Šç»å¯¹åæ ‡ï¼Œå³ä¸Šç›¸å¯¹åæ ‡ï¼Œä¸‹æ–¹æ•°æ®ç»Ÿè®¡
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # è®¡ç®—ç»å¯¹åæ ‡èŒƒå›´
        all_x = np.concatenate([speaker_x, gt_x, pred_x])
        all_y = np.concatenate([speaker_y, gt_y, pred_y])
        x_range = np.max(all_x) - np.min(all_x)
        y_range = np.max(all_y) - np.min(all_y)
        margin = 0.1 * max(x_range, y_range)
        
        # å·¦ä¸Šï¼šç»å¯¹åæ ‡é‡å å›¾
        ax1 = axes[0, 0]
        ax1.set_title('ç¬¬ä¸€å¸§ - ç»å¯¹åæ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.scatter(speaker_x, speaker_y, c='blue', s=30, alpha=0.7, label='Speaker')
        ax1.scatter(gt_x, gt_y, c='green', s=30, alpha=0.7, label='GT Listener')
        ax1.scatter(pred_x, pred_y, c='red', s=30, alpha=0.7, label='Predicted')
        
        if x_range > 1 and y_range > 1:  # åªæœ‰å½“èŒƒå›´åˆç†æ—¶æ‰ç»˜åˆ¶è¿æ¥çº¿
            self.draw_face_connections(ax1, speaker_x, speaker_y, 'blue')
            self.draw_face_connections(ax1, gt_x, gt_y, 'green')
            self.draw_face_connections(ax1, pred_x, pred_y, 'red')
        
        ax1.set_xlim(np.min(all_x) - margin, np.max(all_x) + margin)
        ax1.set_ylim(np.max(all_y) + margin, np.min(all_y) - margin)
        ax1.set_aspect('equal')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³ä¸Šï¼šç›¸å¯¹åæ ‡å¯¹æ¯”
        ax2 = axes[0, 1]
        ax2.set_title('ç¬¬ä¸€å¸§ - ç›¸å¯¹åæ ‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        
        # è½¬æ¢ä¸ºç›¸å¯¹åæ ‡
        def to_relative(x, y):
            center_x, center_y = np.mean(x), np.mean(y)
            return x - center_x, y - center_y
        
        speaker_rel_x, speaker_rel_y = to_relative(speaker_x, speaker_y)
        gt_rel_x, gt_rel_y = to_relative(gt_x, gt_y)
        pred_rel_x, pred_rel_y = to_relative(pred_x, pred_y)
        
        ax2.scatter(speaker_rel_x, speaker_rel_y, c='blue', s=30, alpha=0.7, label='Speaker')
        ax2.scatter(gt_rel_x, gt_rel_y, c='green', s=30, alpha=0.7, label='GT Listener')
        ax2.scatter(pred_rel_x, pred_rel_y, c='red', s=30, alpha=0.7, label='Predicted')
        
        # ç»˜åˆ¶ç›¸å¯¹åæ ‡çš„è¿æ¥çº¿
        self.draw_face_connections(ax2, speaker_rel_x, speaker_rel_y, 'blue')
        self.draw_face_connections(ax2, gt_rel_x, gt_rel_y, 'green')
        self.draw_face_connections(ax2, pred_rel_x, pred_rel_y, 'red')
        
        all_rel = np.concatenate([speaker_rel_x, speaker_rel_y, gt_rel_x, gt_rel_y, pred_rel_x, pred_rel_y])
        max_rel_range = np.max(np.abs(all_rel))
        rel_margin = 0.1 * max_rel_range
        ax2.set_xlim(-max_rel_range - rel_margin, max_rel_range + rel_margin)
        ax2.set_ylim(max_rel_range + rel_margin, -max_rel_range - rel_margin)
        ax2.set_aspect('equal')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)
        ax2.axvline(x=0, color='k', linestyle='-', alpha=0.3)
        
        # å·¦ä¸‹ï¼šæ•°æ®ç»Ÿè®¡è¡¨
        ax3 = axes[1, 0]
        ax3.axis('off')
        ax3.set_title('æ•°æ®ç»Ÿè®¡', fontsize=14, fontweight='bold')
        
        stats_text = f"""
æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼š

ç»å¯¹åæ ‡èŒƒå›´ï¼š
  X: [{np.min(all_x):.2f}, {np.max(all_x):.2f}] (èŒƒå›´: {x_range:.2f})
  Y: [{np.min(all_y):.2f}, {np.max(all_y):.2f}] (èŒƒå›´: {y_range:.2f})

Speakeræ•°æ®ï¼š
  X: [{speaker_x.min():.2f}, {speaker_x.max():.2f}], ä¸­å¿ƒ: {speaker_x.mean():.2f}
  Y: [{speaker_y.min():.2f}, {speaker_y.max():.2f}], ä¸­å¿ƒ: {speaker_y.mean():.2f}

GT Listeneræ•°æ®ï¼š
  X: [{gt_x.min():.2f}, {gt_x.max():.2f}], ä¸­å¿ƒ: {gt_x.mean():.2f}
  Y: [{gt_y.min():.2f}, {gt_y.max():.2f}], ä¸­å¿ƒ: {gt_y.mean():.2f}

Predictedæ•°æ®ï¼š
  X: [{pred_x.min():.2f}, {pred_x.max():.2f}], ä¸­å¿ƒ: {pred_x.mean():.2f}
  Y: [{pred_y.min():.2f}, {pred_y.max():.2f}], ä¸­å¿ƒ: {pred_y.mean():.2f}
        """
        
        ax3.text(0.05, 0.95, stats_text, transform=ax3.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace')
        
        # å³ä¸‹ï¼šè¯¯å·®åˆ†æ
        ax4 = axes[1, 1]
        ax4.set_title('é¢„æµ‹è¯¯å·®åˆ†æ', fontsize=14, fontweight='bold')
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„è¯¯å·®
        errors = np.sqrt((gt_x - pred_x)**2 + (gt_y - pred_y)**2)
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ˜¾ç¤ºè¯¯å·®
        scatter = ax4.scatter(gt_x, gt_y, c=errors, s=50, cmap='Reds', alpha=0.8)
        plt.colorbar(scatter, ax=ax4, label='é¢„æµ‹è¯¯å·® (åƒç´ )')
        
        ax4.set_xlim(gt_x.min() - 10, gt_x.max() + 10)
        ax4.set_ylim(gt_y.max() + 10, gt_y.min() - 10)
        ax4.set_aspect('equal')
        ax4.grid(True, alpha=0.3)
        
        # æ·»åŠ è¯¯å·®ç»Ÿè®¡æ–‡æœ¬
        error_stats = f"å¹³å‡è¯¯å·®: {errors.mean():.2f}\næœ€å¤§è¯¯å·®: {errors.max():.2f}\nè¯¯å·®æ ‡å‡†å·®: {errors.std():.2f}"
        ax4.text(0.02, 0.98, error_stats, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.3", facecolor="white", alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… è°ƒè¯•é™æ€å›¾å·²ä¿å­˜åˆ°: {output_path}")
    
    def draw_face_connections(self, ax, x_coords, y_coords, base_color='blue'):
        """ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿"""
        # 68ç‚¹é¢éƒ¨è¿æ¥çº¿å®šä¹‰
        connections = {
            'jaw': list(range(0, 17)),           # ä¸‹é¢Œçº¿
            'right_eyebrow': list(range(17, 22)), # å³çœ‰æ¯›
            'left_eyebrow': list(range(22, 27)),  # å·¦çœ‰æ¯›
            'nose_bridge': list(range(27, 31)),   # é¼»æ¢
            'nose_tip': list(range(31, 36)),      # é¼»ç¿¼
            'right_eye': list(range(36, 42)) + [36], # å³çœ¼ï¼ˆé—­åˆï¼‰
            'left_eye': list(range(42, 48)) + [42],  # å·¦çœ¼ï¼ˆé—­åˆï¼‰
            'outer_lip': list(range(48, 60)) + [48], # å¤–å˜´å”‡ï¼ˆé—­åˆï¼‰
            'inner_lip': list(range(60, 68)) + [60]  # å†…å˜´å”‡ï¼ˆé—­åˆï¼‰
        }
        
        # æ ¹æ®åŸºç¡€é¢œè‰²ç”Ÿæˆä¸åŒéƒ¨ä½çš„é¢œè‰²
        if base_color == 'blue':
            colors = ['navy', 'blue', 'blue', 'steelblue', 'steelblue', 'darkblue', 'darkblue', 'royalblue', 'royalblue']
        elif base_color == 'green':
            colors = ['darkgreen', 'green', 'green', 'forestgreen', 'forestgreen', 'darkgreen', 'darkgreen', 'limegreen', 'limegreen']
        elif base_color == 'red':
            colors = ['darkred', 'red', 'red', 'crimson', 'crimson', 'darkred', 'darkred', 'orangered', 'orangered']
        else:
            colors = [base_color] * 9
        
        for i, (name, indices) in enumerate(connections.items()):
            if len(indices) > 1:
                try:
                    x_line = [x_coords[idx] for idx in indices]
                    y_line = [y_coords[idx] for idx in indices]
                    ax.plot(x_line, y_line, color=colors[i % len(colors)], 
                           linewidth=1.2, alpha=0.7)
                except IndexError:
                    # å¦‚æœç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œè·³è¿‡è¿™æ¡è¿æ¥çº¿
                    continue
    
    def calculate_metrics(self, gt_listener, predictions):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {}
        
        # å¯¹æ¯ç§ç‰¹å¾è®¡ç®—æŒ‡æ ‡
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            gt_data = gt_listener[feature_name].cpu().numpy().squeeze(0)
            pred_data = predictions[feature_name].cpu().numpy().squeeze(0)
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((gt_data - pred_data) ** 2)
            mae = np.mean(np.abs(gt_data - pred_data))
            rmse = np.sqrt(mse)
            
            metrics[feature_name] = {
                'mse': mse,
                'mae': mae,
                'rmse': rmse
            }
            
            print(f"  {feature_name.upper()}:")
            print(f"    MSE: {mse:.6f}")
            print(f"    MAE: {mae:.6f}")
            print(f"    RMSE: {rmse:.6f}")
        
        # å¯¹landmarksè®¡ç®—æ›´è¯¦ç»†çš„æŒ‡æ ‡
        if 'landmarks' in metrics:
            gt_landmarks = gt_listener['landmarks'].cpu().numpy().squeeze(0)
            pred_landmarks = predictions['landmarks'].cpu().numpy().squeeze(0)
            
            # åˆ†ç¦»xå’Œyåæ ‡çš„è¯¯å·®
            x_errors = np.abs(gt_landmarks[:, :68] - pred_landmarks[:, :68])
            y_errors = np.abs(gt_landmarks[:, 68:] - pred_landmarks[:, 68:])
            
            metrics['landmarks']['x_mae'] = np.mean(x_errors)
            metrics['landmarks']['y_mae'] = np.mean(y_errors)
            metrics['landmarks']['max_point_error'] = np.max(np.sqrt(x_errors**2 + y_errors**2))
            
            print(f"    Xåæ ‡MAE: {metrics['landmarks']['x_mae']:.6f}")
            print(f"    Yåæ ‡MAE: {metrics['landmarks']['y_mae']:.6f}")
            print(f"    æœ€å¤§ç‚¹è¯¯å·®: {metrics['landmarks']['max_point_error']:.6f}")
        
        return metrics
    
    def run_test(self, data_csv, sample_idx=0, output_dir='test_results'):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        
        # 1. åŠ è½½æµ‹è¯•æ ·æœ¬
        batch_data = self.load_test_sample(data_csv, sample_idx)
        
        # 2. ç”Ÿæˆé¢„æµ‹
        predictions, all_predictions = self.generate_predictions(batch_data)
        
        # 3. è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(batch_data['listener'], predictions)
        
        # 4. ä¿å­˜ç»“æœ
        speaker_df, gt_df, pred_df = self.save_results(
            batch_data['speaker'], 
            batch_data['listener'], 
            predictions, 
            output_dir
        )
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        animation_path = os.path.join(output_dir, 'comparison_animation.gif')
        
        # å…ˆåˆ›å»ºé™æ€è°ƒè¯•å›¾
        debug_path = os.path.join(output_dir, 'debug_first_frame.png')
        self.create_debug_static_plot(speaker_df, gt_df, pred_df, debug_path)
        
        # å†åˆ›å»ºåŠ¨ç”»
        self.create_comparison_animation(speaker_df, gt_df, pred_df, animation_path)
        
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return metrics, animation_path, debug_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç®€æ˜“æ¨¡å‹æµ‹è¯•")
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data-csv', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv',
                       help='æµ‹è¯•æ•°æ®CSVè·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='æµ‹è¯•æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--output-dir', type=str, default='test_results_debug',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥checkpointæ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data_csv):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_csv}")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SimpleTester(args.checkpoint, args.device)
        
        # è¿è¡Œæµ‹è¯•
        metrics, animation_path, debug_path = tester.run_test(
            args.data_csv, 
            args.sample_idx, 
            args.output_dir
        )
        
        print("\n" + "="*50)
        print("ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*50)
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"è°ƒè¯•é™æ€å›¾: {debug_path}")
        print(f"åŠ¨ç”»æ–‡ä»¶: {animation_path}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()