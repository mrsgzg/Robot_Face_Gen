#!/usr/bin/env python3
"""
æµå¼æ¨ç†æµ‹è¯•è„šæœ¬ - æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµå¤„ç†
æ”¯æŒçª—å£çº§æµå¼æ¨ç†ã€æ€§èƒ½åˆ†æã€å®æ—¶å¯è§†åŒ–
"""

import os
import time
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.patches import Rectangle
import argparse
from tqdm import tqdm
from collections import deque
from typing import Dict, List, Tuple, Optional
import threading
import queue

from facial_reaction_model.model import FacialReactionModel, OnlineInferenceManager
from scratch.Robot_Face_Gen.Data_Set_old import SpeakerListenerDataset


class StreamingTester:
    """æµå¼æ¨ç†æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.checkpoint_path = checkpoint_path
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_times = []
        self.memory_usage = []
        self.window_delays = []
        
        # æµå¼æ•°æ®ç¼“å­˜
        self.streaming_results = {
            'timestamps': [],
            'predictions': {
                'landmarks': [],
                'au': [],
                'pose': [],
                'gaze': []
            },
            'gt_data': {
                'landmarks': [],
                'au': [],
                'pose': [],
                'gaze': []
            },
            'speaker_data': {
                'landmarks': [],
                'au': [],
                'pose': [],
                'gaze': []
            }
        }
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {self.checkpoint_path}")
        
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        config = checkpoint.get('config', {})
        model_config = config.get('model', {})
        
        self.model = FacialReactionModel(
            feature_dim=model_config.get('feature_dim', 256),
            audio_dim=model_config.get('audio_dim', 384),
            period=model_config.get('period', 25),
            max_seq_len=model_config.get('max_seq_len', 750),
            device=str(self.device),
            window_size=model_config.get('window_size', 16),
            momentum=model_config.get('momentum', 0.9)
        )
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        self.window_size = self.model.window_size
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œçª—å£å¤§å°: {self.window_size}")
        
    def load_test_sample(self, data_csv, sample_idx=0):
        """åŠ è½½æµ‹è¯•æ ·æœ¬"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {data_csv}, æ ·æœ¬ç´¢å¼•: {sample_idx}")
        
        dataset = SpeakerListenerDataset(data_csv)
        
        if sample_idx >= len(dataset):
            sample_idx = 0
            print(f"âš ï¸ æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨ç´¢å¼• 0")
        
        sample = dataset[sample_idx]
        if sample is None:
            raise ValueError(f"æ— æ³•åŠ è½½æ ·æœ¬ {sample_idx}")
        
        # ä¸è½¬æ¢ä¸ºbatchæ ¼å¼ï¼Œä¿æŒåŸå§‹å½¢çŠ¶ç”¨äºæµå¼å¤„ç†
        return sample
    
    def simulate_data_stream(self, sample_data, stream_fps=25, real_time=False):
        """
        æ¨¡æ‹Ÿæ•°æ®æµ
        Args:
            sample_data: å®Œæ•´çš„æ ·æœ¬æ•°æ®
            stream_fps: æµçš„å¸§ç‡
            real_time: æ˜¯å¦æŒ‰çœŸå®æ—¶é—´æµå¼å¤„ç†
        Returns:
            ç”Ÿæˆå™¨ï¼Œäº§å‡ºæ¯ä¸ªçª—å£çš„æ•°æ®
        """
        speaker_data = sample_data['speaker']
        listener_data = sample_data['listener']
        
        total_frames = speaker_data['landmarks'].shape[0]
        frame_interval = 1.0 / stream_fps if real_time else 0
        
        print(f"ğŸ¬ å¼€å§‹æ¨¡æ‹Ÿæ•°æ®æµ: {total_frames}å¸§, {stream_fps}fps")
        
        # æŒ‰çª—å£ç”Ÿæˆæ•°æ®
        for start_idx in range(0, total_frames, self.window_size):
            end_idx = min(start_idx + self.window_size, total_frames)
            window_size = end_idx - start_idx
            
            # å‡†å¤‡å½“å‰çª—å£çš„æ•°æ®
            window_speaker = {}
            window_listener = {}
            
            for feature_name in ['landmarks', 'au', 'pose', 'gaze', 'audio']:
                if feature_name in speaker_data:
                    window_speaker[feature_name] = speaker_data[feature_name][start_idx:end_idx]
                if feature_name in listener_data:
                    window_listener[feature_name] = listener_data[feature_name][start_idx:end_idx]
            
            # è½¬æ¢ä¸ºbatchæ ¼å¼ä¾›æ¨¡å‹ä½¿ç”¨
            batch_speaker = {}
            for key, value in window_speaker.items():
                batch_speaker[key] = value.unsqueeze(0).to(self.device)
            
            yield {
                'window_idx': start_idx // self.window_size,
                'start_frame': start_idx,
                'end_frame': end_idx,
                'window_size': window_size,
                'speaker_data': batch_speaker,
                'gt_listener': window_listener,
                'timestamp': time.time()
            }
            
            # å®æ—¶æ¨¡å¼ä¸‹æŒ‰å¸§ç‡ç­‰å¾…
            if real_time and window_size == self.window_size:
                time.sleep(frame_interval * self.window_size)
    
    def streaming_inference(self, sample_data, output_dir='streaming_results', 
                          stream_fps=25, real_time=False, max_windows=None):
        """
        æ‰§è¡Œæµå¼æ¨ç†
        Args:
            sample_data: æµ‹è¯•æ ·æœ¬æ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            stream_fps: æµå¸§ç‡
            real_time: æ˜¯å¦å®æ—¶å¤„ç†
            max_windows: æœ€å¤§å¤„ç†çª—å£æ•°ï¼ˆNoneä¸ºå…¨éƒ¨ï¼‰
        """
        print(f"ğŸš€ å¼€å§‹æµå¼æ¨ç† (å®æ—¶æ¨¡å¼: {real_time})")
        
        os.makedirs(output_dir, exist_ok=True)
        
        # åˆ›å»ºåœ¨çº¿æ¨ç†ç®¡ç†å™¨
        online_manager = OnlineInferenceManager(self.model, self.device)
        
        # é‡ç½®ç»Ÿè®¡ä¿¡æ¯
        self.inference_times.clear()
        self.memory_usage.clear()
        self.window_delays.clear()
        
        # é‡ç½®æµå¼ç»“æœ
        for key in self.streaming_results:
            if key == 'predictions' or key == 'gt_data' or key == 'speaker_data':
                for feature in self.streaming_results[key]:
                    self.streaming_results[key][feature].clear()
            else:
                self.streaming_results[key].clear()
        
        processed_windows = 0
        
        # å¼€å§‹æµå¼å¤„ç†
        for window_data in self.simulate_data_stream(sample_data, stream_fps, real_time):
            if max_windows and processed_windows >= max_windows:
                break
                
            window_idx = window_data['window_idx']
            start_frame = window_data['start_frame']
            
            print(f"ğŸ¯ å¤„ç†çª—å£ {window_idx}: å¸§ {start_frame}-{window_data['end_frame']}")
            
            # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
            inference_start = time.time()
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated() / 1024**2
            
            # æ‰§è¡Œæ¨ç†
            with torch.no_grad():
                window_predictions = online_manager.process_window(window_data['speaker_data'])
            
            # è®°å½•æ¨ç†æ—¶é—´
            inference_time = time.time() - inference_start
            self.inference_times.append(inference_time)
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated() / 1024**2
                self.memory_usage.append(memory_after - memory_before)
            
            # è®¡ç®—çª—å£å»¶è¿Ÿ
            window_delay = window_data['timestamp'] - inference_start
            self.window_delays.append(window_delay)
            
            # ä¿å­˜é¢„æµ‹ç»“æœ
            self.save_window_results(
                window_data, window_predictions, 
                window_idx, start_frame
            )
            
            processed_windows += 1
            
            # å®æ—¶æ˜¾ç¤ºè¿›åº¦
            if processed_windows % 5 == 0:
                avg_inference_time = np.mean(self.inference_times[-5:])
                print(f"    ğŸ“Š æœ€è¿‘5çª—å£å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time*1000:.2f}ms")
        
        print(f"âœ… æµå¼æ¨ç†å®Œæˆï¼Œå¤„ç†äº† {processed_windows} ä¸ªçª—å£")
        
        # ä¿å­˜æ€§èƒ½ç»Ÿè®¡
        self.save_performance_stats(output_dir)
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        self.save_complete_results(output_dir)
        
        return processed_windows
    
    def save_window_results(self, window_data, predictions, window_idx, start_frame):
        """ä¿å­˜å•ä¸ªçª—å£çš„ç»“æœ"""
        # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
        pred_denorm = self.denormalize_features({
            k: v.cpu().numpy().squeeze(0) for k, v in predictions.items()
        })
        
        # åå½’ä¸€åŒ–çœŸå®å€¼
        gt_denorm = self.denormalize_features(window_data['gt_listener'])
        
        # åå½’ä¸€åŒ–speakeræ•°æ®
        speaker_denorm = self.denormalize_features({
            k: v.cpu().numpy().squeeze(0) if torch.is_tensor(v) else v.numpy()
            for k, v in window_data['speaker_data'].items() if k != 'audio'
        })
        
        # ä¿å­˜åˆ°æµå¼ç»“æœä¸­
        self.streaming_results['timestamps'].append(window_data['timestamp'])
        
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            if feature_name in pred_denorm:
                self.streaming_results['predictions'][feature_name].append(pred_denorm[feature_name])
            if feature_name in gt_denorm:
                self.streaming_results['gt_data'][feature_name].append(gt_denorm[feature_name])
            if feature_name in speaker_denorm:
                self.streaming_results['speaker_data'][feature_name].append(speaker_denorm[feature_name])
    
    def denormalize_features(self, data_dict):
        """åå½’ä¸€åŒ–ç‰¹å¾"""
        result = {}
        for key, value in data_dict.items():
            if key == 'landmarks':
                result[key] = value * 256.0
            elif key == 'au':
                result[key] = value * 5.0
            else:  # pose, gazeä¿æŒåŸå€¼
                result[key] = value
        return result
    
    def save_performance_stats(self, output_dir):
        """ä¿å­˜æ€§èƒ½ç»Ÿè®¡"""
        stats = {
            'total_windows': len(self.inference_times),
            'total_inference_time': sum(self.inference_times),
            'avg_inference_time': np.mean(self.inference_times),
            'std_inference_time': np.std(self.inference_times),
            'min_inference_time': np.min(self.inference_times),
            'max_inference_time': np.max(self.inference_times),
            'window_size': self.window_size,
            'theoretical_delay': self.window_size / 25.0,  # å‡è®¾25fps
        }
        
        if self.memory_usage:
            stats.update({
                'avg_memory_usage': np.mean(self.memory_usage),
                'max_memory_usage': np.max(self.memory_usage),
            })
        
        # ä¿å­˜ç»Ÿè®¡JSON
        import json
        stats_path = os.path.join(output_dir, 'performance_stats.json')
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=2)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»çª—å£æ•°: {stats['total_windows']}")
        print(f"  å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']*1000:.2f}ms")
        print(f"  æ¨ç†æ—¶é—´æ ‡å‡†å·®: {stats['std_inference_time']*1000:.2f}ms")
        print(f"  æœ€å°æ¨ç†æ—¶é—´: {stats['min_inference_time']*1000:.2f}ms")
        print(f"  æœ€å¤§æ¨ç†æ—¶é—´: {stats['max_inference_time']*1000:.2f}ms")
        print(f"  ç†è®ºå»¶è¿Ÿ: {stats['theoretical_delay']*1000:.0f}ms ({self.window_size}å¸§@25fps)")
        
        if self.memory_usage:
            print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {stats['avg_memory_usage']:.2f}MB")
            print(f"  æœ€å¤§å†…å­˜ä½¿ç”¨: {stats['max_memory_usage']:.2f}MB")
        
        # åˆ›å»ºæ€§èƒ½å›¾è¡¨
        self.create_performance_plots(output_dir)
    
    def save_complete_results(self, output_dir):
        """ä¿å­˜å®Œæ•´çš„æµå¼ç»“æœ"""
        print("ğŸ’¾ ä¿å­˜å®Œæ•´æµå¼ç»“æœ...")
        
        # å°†çª—å£çº§ç»“æœæ‹¼æ¥æˆå®Œæ•´åºåˆ—
        complete_results = {}
        
        for role in ['predictions', 'gt_data', 'speaker_data']:
            complete_results[role] = {}
            for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
                if self.streaming_results[role][feature_name]:
                    # æ‹¼æ¥æ‰€æœ‰çª—å£çš„ç»“æœ
                    feature_data = np.concatenate(
                        self.streaming_results[role][feature_name], axis=0
                    )
                    complete_results[role][feature_name] = feature_data
        
        # ä¿å­˜ä¸ºCSVæ–‡ä»¶
        for role in complete_results:
            for feature_name, feature_data in complete_results[role].items():
                filename = os.path.join(output_dir, f'streaming_{role}_{feature_name}.csv')
                
                if feature_name == 'landmarks':
                    columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
                elif feature_name == 'au':
                    columns = [f'AU_{i}' for i in range(feature_data.shape[1])]
                elif feature_name == 'pose':
                    columns = ['pose_Rx', 'pose_Ry', 'pose_Rz']
                elif feature_name == 'gaze':
                    columns = ['gaze_angle_x', 'gaze_angle_y']
                
                df = pd.DataFrame(feature_data, columns=columns)
                df.insert(0, 'frame', range(len(df)))
                df.to_csv(filename, index=False)
        
        return complete_results
    
    def create_performance_plots(self, output_dir):
        """åˆ›å»ºæ€§èƒ½åˆ†æå›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # æ¨ç†æ—¶é—´è¶‹åŠ¿
        ax1 = axes[0, 0]
        ax1.plot(self.inference_times, 'b-', alpha=0.7)
        ax1.axhline(y=np.mean(self.inference_times), color='r', linestyle='--', 
                   label=f'å¹³å‡å€¼: {np.mean(self.inference_times)*1000:.2f}ms')
        ax1.set_title('æ¨ç†æ—¶é—´è¶‹åŠ¿')
        ax1.set_xlabel('çª—å£ç´¢å¼•')
        ax1.set_ylabel('æ¨ç†æ—¶é—´ (ç§’)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # æ¨ç†æ—¶é—´åˆ†å¸ƒ
        ax2 = axes[0, 1]
        ax2.hist(np.array(self.inference_times) * 1000, bins=20, alpha=0.7, edgecolor='black')
        ax2.axvline(x=np.mean(self.inference_times) * 1000, color='r', linestyle='--',
                   label=f'å¹³å‡å€¼: {np.mean(self.inference_times)*1000:.2f}ms')
        ax2.set_title('æ¨ç†æ—¶é—´åˆ†å¸ƒ')
        ax2.set_xlabel('æ¨ç†æ—¶é—´ (æ¯«ç§’)')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # å†…å­˜ä½¿ç”¨ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        ax3 = axes[1, 0]
        if self.memory_usage:
            ax3.plot(self.memory_usage, 'g-', alpha=0.7)
            ax3.axhline(y=np.mean(self.memory_usage), color='r', linestyle='--',
                       label=f'å¹³å‡å€¼: {np.mean(self.memory_usage):.2f}MB')
            ax3.set_title('å†…å­˜ä½¿ç”¨è¶‹åŠ¿')
            ax3.set_xlabel('çª—å£ç´¢å¼•')
            ax3.set_ylabel('å†…å­˜ä½¿ç”¨ (MB)')
            ax3.legend()
        else:
            ax3.text(0.5, 0.5, 'æ— å†…å­˜ä½¿ç”¨æ•°æ®', ha='center', va='center',
                    transform=ax3.transAxes, fontsize=14)
            ax3.set_title('å†…å­˜ä½¿ç”¨è¶‹åŠ¿')
        ax3.grid(True, alpha=0.3)
        
        # å»¶è¿Ÿåˆ†æ
        ax4 = axes[1, 1]
        theoretical_delay = self.window_size / 25.0 * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        actual_delays = np.array(self.inference_times) * 1000
        
        ax4.plot(actual_delays, 'b-', alpha=0.7, label='å®é™…æ¨ç†æ—¶é—´')
        ax4.axhline(y=theoretical_delay, color='r', linestyle='--',
                   label=f'ç†è®ºå»¶è¿Ÿ: {theoretical_delay:.0f}ms')
        ax4.axhline(y=np.mean(actual_delays), color='g', linestyle='--',
                   label=f'å¹³å‡æ¨ç†æ—¶é—´: {np.mean(actual_delays):.2f}ms')
        ax4.set_title('å»¶è¿Ÿåˆ†æ')
        ax4.set_xlabel('çª—å£ç´¢å¼•')
        ax4.set_ylabel('æ—¶é—´ (æ¯«ç§’)')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'performance_analysis.png'), dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… æ€§èƒ½åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: {os.path.join(output_dir, 'performance_analysis.png')}")
    
    def create_streaming_animation(self, output_dir, max_frames=300):
        """åˆ›å»ºæµå¼å¤„ç†åŠ¨ç”»"""
        print("ğŸ¬ åˆ›å»ºæµå¼å¤„ç†åŠ¨ç”»...")
        
        # è·å–å®Œæ•´ç»“æœ
        pred_landmarks = np.concatenate(self.streaming_results['predictions']['landmarks'], axis=0)
        gt_landmarks = np.concatenate(self.streaming_results['gt_data']['landmarks'], axis=0)
        speaker_landmarks = np.concatenate(self.streaming_results['speaker_data']['landmarks'], axis=0)
        
        n_frames = min(len(pred_landmarks), max_frames)
        
        # æå–åæ ‡
        def extract_coords(landmarks):
            x_coords = landmarks[:n_frames, :68]
            y_coords = landmarks[:n_frames, 68:]
            return x_coords, y_coords
        
        pred_x, pred_y = extract_coords(pred_landmarks)
        gt_x, gt_y = extract_coords(gt_landmarks)
        speaker_x, speaker_y = extract_coords(speaker_landmarks)
        
        # è®¡ç®—åæ ‡èŒƒå›´
        all_x = np.concatenate([pred_x.flatten(), gt_x.flatten(), speaker_x.flatten()])
        all_y = np.concatenate([pred_y.flatten(), gt_y.flatten(), speaker_y.flatten()])
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        margin = 0.1 * max(x_max - x_min, y_max - y_min)
        
        # åˆ›å»ºåŠ¨ç”»
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        def animate(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in axes.flat:
                ax.clear()
            
            # å½“å‰çª—å£ä¿¡æ¯
            current_window = frame // self.window_size
            frame_in_window = frame % self.window_size
            
            # å­å›¾1: Speaker
            ax1 = axes[0, 0]
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7)
            ax1.set_title(f'Speaker - å¸§ {frame+1}', fontsize=12)
            ax1.set_xlim(x_min - margin, x_max + margin)
            ax1.set_ylim(y_max + margin, y_min - margin)
            ax1.set_aspect('equal')
            self.draw_face_connections(ax1, speaker_x[frame], speaker_y[frame], 'blue')
            
            # å­å›¾2: GT Listener
            ax2 = axes[0, 1]
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7)
            ax2.set_title(f'GT Listener - å¸§ {frame+1}', fontsize=12)
            ax2.set_xlim(x_min - margin, x_max + margin)
            ax2.set_ylim(y_max + margin, y_min - margin)
            ax2.set_aspect('equal')
            self.draw_face_connections(ax2, gt_x[frame], gt_y[frame], 'green')
            
            # å­å›¾3: Predicted Listener
            ax3 = axes[1, 0]
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7)
            ax3.set_title(f'Predicted Listener - å¸§ {frame+1}', fontsize=12)
            ax3.set_xlim(x_min - margin, x_max + margin)
            ax3.set_ylim(y_max + margin, y_min - margin)
            ax3.set_aspect('equal')
            self.draw_face_connections(ax3, pred_x[frame], pred_y[frame], 'red')
            
            # å­å›¾4: æµå¼å¤„ç†çŠ¶æ€
            ax4 = axes[1, 1]
            ax4.set_xlim(0, 10)
            ax4.set_ylim(0, 10)
            ax4.set_title(f'æµå¼å¤„ç†çŠ¶æ€', fontsize=12)
            
            # æ˜¾ç¤ºå½“å‰çª—å£ä¿¡æ¯
            status_text = f"""
çª—å£ {current_window + 1}
å¸§ {frame_in_window + 1}/{self.window_size}
æ€»å¸§æ•°: {frame + 1}/{n_frames}

çª—å£å¤§å°: {self.window_size}
å»¶è¿Ÿ: ~{self.window_size/25*1000:.0f}ms
            """
            ax4.text(0.1, 0.7, status_text, fontsize=10, verticalalignment='top',
                    fontfamily='monospace')
            
            # ç»˜åˆ¶çª—å£è¿›åº¦æ¡
            window_progress = frame_in_window / self.window_size
            progress_bar = Rectangle((0.5, 2), 8, 1, facecolor='lightblue', edgecolor='black')
            ax4.add_patch(progress_bar)
            
            filled_bar = Rectangle((0.5, 2), 8 * window_progress, 1, 
                                 facecolor='blue', edgecolor='black')
            ax4.add_patch(filled_bar)
            
            ax4.text(4.5, 1.5, f'çª—å£è¿›åº¦: {window_progress*100:.1f}%', 
                    ha='center', fontsize=10)
            
            ax4.set_xticks([])
            ax4.set_yticks([])
        
        # åˆ›å»ºåŠ¨ç”»
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, 
                                     interval=100, repeat=True)
        
        # ä¿å­˜åŠ¨ç”»
        animation_path = os.path.join(output_dir, 'streaming_animation.gif')
        anim.save(animation_path, writer='pillow', fps=10, dpi=80)
        plt.close()
        
        print(f"âœ… æµå¼å¤„ç†åŠ¨ç”»å·²ä¿å­˜åˆ°: {animation_path}")
        return animation_path
    
    def draw_face_connections(self, ax, x_coords, y_coords, color='blue'):
        """ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿"""
        connections = {
            'jaw': list(range(0, 17)),
            'right_eyebrow': list(range(17, 22)),
            'left_eyebrow': list(range(22, 27)),
            'nose_bridge': list(range(27, 31)),
            'nose_tip': list(range(31, 36)),
            'right_eye': list(range(36, 42)) + [36],
            'left_eye': list(range(42, 48)) + [42],
            'outer_lip': list(range(48, 60)) + [48],
            'inner_lip': list(range(60, 68)) + [60]
        }
        
        for name, indices in connections.items():
            if len(indices) > 1:
                try:
                    x_line = [x_coords[idx] for idx in indices]
                    y_line = [y_coords[idx] for idx in indices]
                    ax.plot(x_line, y_line, color=color, linewidth=1, alpha=0.6)
                except IndexError:
                    continue
    
    def compare_streaming_vs_batch(self, sample_data, output_dir):
        """å¯¹æ¯”æµå¼æ¨ç†å’Œæ‰¹é‡æ¨ç†çš„ç»“æœ"""
        print("ğŸ”„ å¯¹æ¯”æµå¼æ¨ç† vs æ‰¹é‡æ¨ç†...")
        
        # æ‰¹é‡æ¨ç†
        batch_start = time.time()
        
        batch_speaker = {}
        for key, value in sample_data['speaker'].items():
            batch_speaker[key] = value.unsqueeze(0).to(self.device)
        
        with torch.no_grad():
            batch_predictions, _, _ = self.model(batch_speaker, speaker_out=False)
        
        batch_time = time.time() - batch_start
        
        # åå½’ä¸€åŒ–æ‰¹é‡ç»“æœ
        batch_denorm = self.denormalize_features({
            k: v.cpu().numpy().squeeze(0) for k, v in batch_predictions.items()
        })
        
        # è·å–æµå¼ç»“æœ
        streaming_denorm = {}
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            if self.streaming_results['predictions'][feature_name]:
                streaming_denorm[feature_name] = np.concatenate(
                    self.streaming_results['predictions'][feature_name], axis=0
                )
        
        # è®¡ç®—å·®å¼‚
        comparison_stats = {}
        for feature_name in streaming_denorm:
            if feature_name in batch_denorm:
                # ç¡®ä¿é•¿åº¦ä¸€è‡´
                min_len = min(len(streaming_denorm[feature_name]), 
                             len(batch_denorm[feature_name]))
                
                stream_data = streaming_denorm[feature_name][:min_len]
                batch_data = batch_denorm[feature_name][:min_len]
                
                # è®¡ç®—å·®å¼‚
                diff = np.abs(stream_data - batch_data)
                comparison_stats[feature_name] = {
                    'max_diff': np.max(diff),
                    'mean_diff': np.mean(diff),
                    'std_diff': np.std(diff)
                }
        
        # æ—¶é—´å¯¹æ¯”
        streaming_total_time = sum(self.inference_times)
        
        print(f"\nâš–ï¸ æµå¼ vs æ‰¹é‡æ¨ç†å¯¹æ¯”:")
        print(f"  æ‰¹é‡æ¨ç†æ—¶é—´: {batch_time*1000:.2f}ms")
        print(f"  æµå¼æ¨ç†æ€»æ—¶é—´: {streaming_total_time*1000:.2f}ms")
        print(f"  æ—¶é—´æ¯”ç‡: {streaming_total_time/batch_time:.2f}x")
        
        print(f"\nğŸ“Š é¢„æµ‹å·®å¼‚åˆ†æ:")
        for feature_name, stats in comparison_stats.items():
            print(f"  {feature_name.upper()}:")
            print(f"    æœ€å¤§å·®å¼‚: {stats['max_diff']:.6f}")
            print(f"    å¹³å‡å·®å¼‚: {stats['mean_diff']:.6f}")
            print(f"    å·®å¼‚æ ‡å‡†å·®: {stats['std_diff']:.6f}")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_data = {
            'batch_inference_time': str(batch_time),
            'streaming_total_time': str(streaming_total_time),
            'time_ratio': str(streaming_total_time / batch_time),
            'feature_differences': str(comparison_stats)
        }
        
        import json
        comparison_path = os.path.join(output_dir, 'streaming_vs_batch_comparison.json')
        with open(comparison_path, 'w') as f:
            json.dump(comparison_data, f, indent=2)
        
        return comparison_data
    
    def run_streaming_test(self, data_csv, sample_idx=0, output_dir='streaming_results',
                          stream_fps=25, real_time=False, max_windows=None):
        """è¿è¡Œå®Œæ•´çš„æµå¼æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æµå¼æ¨ç†æµ‹è¯•...")
        
        # 1. åŠ è½½æµ‹è¯•æ ·æœ¬
        sample_data = self.load_test_sample(data_csv, sample_idx)
        
        # 2. æ‰§è¡Œæµå¼æ¨ç†
        processed_windows = self.streaming_inference(
            sample_data, output_dir, stream_fps, real_time, max_windows
        )
        
        # 3. å¯¹æ¯”æµå¼å’Œæ‰¹é‡æ¨ç†
        comparison_data = self.compare_streaming_vs_batch(sample_data, output_dir)
        
        # 4. åˆ›å»ºå¯è§†åŒ–
        animation_path = self.create_streaming_animation(output_dir)
        
        print("ğŸ‰ æµå¼æµ‹è¯•å®Œæˆï¼")
        
        return {
            'processed_windows': processed_windows,
            'animation_path': animation_path,
            'comparison_data': comparison_data,
            'output_dir': output_dir
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æµå¼æ¨ç†æµ‹è¯•")
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data-csv', type=str,
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv',
                       help='æµ‹è¯•æ•°æ®CSVè·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='æµ‹è¯•æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--output-dir', type=str, default='streaming_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    parser.add_argument('--stream-fps', type=int, default=25,
                       help='æµå¸§ç‡ (é»˜è®¤: 25)')
    parser.add_argument('--real-time', action='store_true',
                       help='å¯ç”¨å®æ—¶å¤„ç†æ¨¡å¼')
    parser.add_argument('--max-windows', type=int, default=None,
                       help='æœ€å¤§å¤„ç†çª—å£æ•°')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    if not os.path.exists(args.data_csv):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_csv}")
        return
    
    try:
        # åˆ›å»ºæµå¼æµ‹è¯•å™¨
        tester = StreamingTester(args.checkpoint, args.device)
        
        # è¿è¡Œæµå¼æµ‹è¯•
        results = tester.run_streaming_test(
            args.data_csv,
            args.sample_idx,
            args.output_dir,
            args.stream_fps,
            args.real_time,
            args.max_windows
        )
        
        print("\n" + "="*60)
        print("ğŸ¯ æµå¼æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*60)
        print(f"è¾“å‡ºç›®å½•: {results['output_dir']}")
        print(f"å¤„ç†çª—å£æ•°: {results['processed_windows']}")
        print(f"åŠ¨ç”»æ–‡ä»¶: {results['animation_path']}")
        print(f"æ—¶é—´æ¯”ç‡ (æµå¼/æ‰¹é‡): {float(results['comparison_data']['time_ratio']):.2f}x")

        print("="*60)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()