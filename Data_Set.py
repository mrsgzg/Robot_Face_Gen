import os
import numpy as np
import pandas as pd
import torch
import torchaudio
import soundfile as sf
from torch.utils.data import Dataset, DataLoader
from transformers import WhisperFeatureExtractor, WhisperModel
import warnings
warnings.filterwarnings('ignore')

class SpeakerListenerDataset(Dataset):
    def __init__(self, mapping_csv, whisper_model_name="openai/whisper-tiny"):
        self.target_length = 750  # âœ… ç»Ÿä¸€é•¿åº¦ä¸º 750 å¸§
        self.mapping_df = pd.read_csv(mapping_csv, engine="c")  # âœ… ä½¿ç”¨ C è§£æå™¨åŠ é€Ÿ
        
        # **ğŸ”¹ å®šä¹‰OpenFaceç‰¹å¾åˆ—ç´¢å¼•**
        self.landmark_x_cols = list(range(18, 69))        # ç‚¹17-67çš„xåæ ‡ (åˆ—18-68)
        self.landmark_y_cols = list(range(86, 137)) 
        #self.landmark_cols = list(range(1, 137))      # Face landmarks (68x + 68y)
        self.au_cols = list(range(137, 154))          # Face AU 
        self.pose_cols = list(range(157, 160))        # Head pose
        self.gaze_cols = list(range(160, 162))        # Gaze angle
        
        # **ğŸ”¹ åŠ è½½Whisperæ¨¡å‹ç”¨äºéŸ³é¢‘ç‰¹å¾æå–**
        print(f"ğŸµ Loading Whisper model: {whisper_model_name}")
        self.whisper_feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)
        self.whisper_model = WhisperModel.from_pretrained(whisper_model_name)
        self.whisper_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # è·å–Whisperæ¨¡å‹çš„é‡‡æ ·ç‡
        self.whisper_sampling_rate = self.whisper_feature_extractor.sampling_rate
        print(f"âœ… Whisper model loaded, sampling rate: {self.whisper_sampling_rate}")

    def _convert_path(self, path, ext):
        """è‡ªåŠ¨é€‚é… Windows å’Œ Ubuntu è·¯å¾„ï¼ŒåŒæ—¶æ›¿æ¢æ‰©å±•å"""
        path = "/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/Face/"+path+"."+ext
        return path
    
    def _convert_audio_path(self, path, ext):
        path = "/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/Audio_files/"+path+"."+ext
        return path
    #/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data
    def _interpolate_missing_frames(self, df):
        """å¡«å……ç¼ºå¤±å¸§ï¼Œç¡®ä¿ 750 å¸§"""
        if "frame" not in df.columns:
            return df  # è‹¥æ—  frame åˆ—ï¼Œåˆ™ç›´æ¥è¿”å›
        if len(df) == self.target_length:
            return df

        full_frame_range = np.arange(0, self.target_length)  # ç”Ÿæˆ 0~749 å¸§
        df = df.set_index("frame").reindex(full_frame_range)  # é‡æ–°ç´¢å¼•
        df.interpolate(method="linear", inplace=True)  # çº¿æ€§æ’å€¼
        df.ffill(inplace=True)  # å‘å‰å¡«å……
        df.bfill(inplace=True)  # å‘åå¡«å……

        return df.reset_index()

    def _load_parquet(self, parquet_path):
        """è¯»å– Parquet å¹¶æŒ‰ç‰¹å¾ç±»å‹åˆ†ç¦»ï¼Œåªå¯¹landmarksè¿›è¡Œå½’ä¸€åŒ–"""
        if not os.path.exists(parquet_path) or os.stat(parquet_path).st_size == 0:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º: {parquet_path}")
            return None, None, None, None

        try:
            df = pd.read_parquet(parquet_path)  # è¯»å– Parquet
            #print("load_successfully")
            df = df.loc[:self.target_length - 1]  # âœ… é™åˆ¶ä¸º 750 å¸§
            df = self._interpolate_missing_frames(df)  # **âœ… æ’å€¼è¡¥å¸§**
            
            landmarks_x = df.iloc[:, self.landmark_x_cols].values  # (750, 51) - 51ä¸ªç‚¹çš„xåæ ‡
            landmarks_y = df.iloc[:, self.landmark_y_cols].values  # (750, 51) - 51ä¸ªç‚¹çš„yåæ ‡
            
            # **ğŸ”¹ æ–°å¢ï¼šlandmarksä¸­å¿ƒåŒ–å¤„ç†**
            landmarks_x_centered, landmarks_y_centered = self._center_landmarks(landmarks_x, landmarks_y)
            
            # åˆå¹¶ä¸ºä¸€ä¸ªæ•°ç»„ (750, 102) - 51ä¸ªç‚¹çš„x,yåæ ‡
            landmarks = np.concatenate([landmarks_x_centered, landmarks_y_centered], axis=1)
            
            # 2. Face AU (137-154åˆ—ï¼Œç´¢å¼•136-153) - ä¿æŒåŸå§‹å€¼
            au_features = df.iloc[:, self.au_cols].values / 5.0
            
            # 3. Head pose (157-160åˆ—ï¼Œç´¢å¼•156-159) - ä¿æŒåŸå§‹å€¼
            pose_features = df.iloc[:, self.pose_cols].values / np.pi
            
            # 4. Gaze angle (161-162åˆ—ï¼Œç´¢å¼•160-161) - ä¿æŒåŸå§‹å€¼
            gaze_features = df.iloc[:, self.gaze_cols].values  / np.pi

            return landmarks, au_features, pose_features, gaze_features
            
        except Exception as e:
            print(f"âš ï¸  åŠ è½½å¤±è´¥: {parquet_path} - {e}")
            return None, None, None, None
    def _center_landmarks(self, landmarks_x, landmarks_y):
        """
        å¯¹landmarksè¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†
        Args:
            landmarks_x: (T, 51) - 51ä¸ªç‚¹çš„xåæ ‡  
            landmarks_y: (T, 51) - 51ä¸ªç‚¹çš„yåæ ‡
        Returns:
            tuple: (centered_x, centered_y) - ä¸­å¿ƒåŒ–åçš„åæ ‡
        """
        centered_x = np.zeros_like(landmarks_x)
        centered_y = np.zeros_like(landmarks_y)
        
        for frame_idx in range(landmarks_x.shape[0]):
            # è®¡ç®—å½“å‰å¸§æ‰€æœ‰ç‚¹çš„è´¨å¿ƒ
            center_x = np.mean(landmarks_x[frame_idx])
            center_y = np.mean(landmarks_y[frame_idx])
            
            # ä¸­å¿ƒåŒ–ï¼šå‡å»è´¨å¿ƒ
            centered_x[frame_idx] = landmarks_x[frame_idx] - center_x
            centered_y[frame_idx] = landmarks_y[frame_idx] - center_y
        
        # å½’ä¸€åŒ–ï¼šé™¤ä»¥ä¸€ä¸ªå°ºåº¦å› å­æ¥æ§åˆ¶æ•°å€¼èŒƒå›´
        # å¯ä»¥ä½¿ç”¨æ ‡å‡†å·®æˆ–è€…å›ºå®šçš„å°ºåº¦å› å­
        scale_factor = 224  # å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        centered_x = centered_x / scale_factor
        centered_y = centered_y / scale_factor
        
        return centered_x, centered_y
    
    def _load_audio_with_whisper(self, audio_path):
        """ä½¿ç”¨Whisperæå–éŸ³é¢‘ç‰¹å¾"""
        if not os.path.exists(audio_path):
            print(f"âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            return None

        try:
            # 1ï¸âƒ£ **è¯»å–éŸ³é¢‘æ–‡ä»¶**
            waveform, sr = torchaudio.load(audio_path)
            
            # è½¬æ¢ä¸ºå•å£°é“
            if waveform.shape[0] > 1:
                waveform = waveform.mean(dim=0, keepdim=True)
            
            # è½¬æ¢ä¸ºnumpyå¹¶é‡é‡‡æ ·åˆ°Whisperæ‰€éœ€çš„é‡‡æ ·ç‡
            audio_array = waveform.squeeze().numpy()
            
            # é‡é‡‡æ ·åˆ°Whisperçš„é‡‡æ ·ç‡ (16kHz)
            if sr != self.whisper_sampling_rate:
                resampler = torchaudio.transforms.Resample(sr, self.whisper_sampling_rate)
                audio_tensor = torch.from_numpy(audio_array).unsqueeze(0)
                audio_array = resampler(audio_tensor).squeeze().numpy()
            
            # 2ï¸âƒ£ **ä½¿ç”¨Whisperç‰¹å¾æå–å™¨å¤„ç†éŸ³é¢‘**
            # WhisperæœŸæœ›éŸ³é¢‘é•¿åº¦ä¸º30ç§’ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿éŸ³é¢‘é•¿åº¦é€‚åˆ
            max_length = 30 * self.whisper_sampling_rate  # 30ç§’çš„æ ·æœ¬æ•°
            if len(audio_array) > max_length:
                # å¦‚æœéŸ³é¢‘è¶…è¿‡30ç§’ï¼Œæˆªæ–­åˆ°30ç§’
                audio_array = audio_array[:max_length]
            
            # ä½¿ç”¨ç‰¹å¾æå–å™¨å¤„ç†éŸ³é¢‘
            inputs = self.whisper_feature_extractor(
                audio_array, 
                sampling_rate=self.whisper_sampling_rate,
                return_tensors="pt"
            )
            
            # 3ï¸âƒ£ **é€šè¿‡Whisperç¼–ç å™¨æå–ç‰¹å¾**
            with torch.no_grad():
                # è·å–ç¼–ç å™¨è¾“å‡º
                encoder_outputs = self.whisper_model.encoder(
                    inputs.input_features,
                    return_dict=True
                )
                
                # è·å–æœ€åä¸€å±‚çš„éšè—çŠ¶æ€
                audio_features = encoder_outputs.last_hidden_state  # shape: (1, time_steps, hidden_size)
                audio_features = audio_features.squeeze(0)  # ç§»é™¤batchç»´åº¦: (time_steps, hidden_size)
                audio_features = audio_features.numpy()
            
            # 4ï¸âƒ£ **è°ƒæ•´ç‰¹å¾é•¿åº¦ä»¥åŒ¹é…è§†é¢‘å¸§æ•° (750å¸§)**
            current_frames = audio_features.shape[0]
            target_frames = self.target_length
            
            if current_frames < target_frames:
                # å¦‚æœç‰¹å¾å¸§æ•°ä¸è¶³ï¼Œä½¿ç”¨é‡å¤å¡«å……
                repeat_factor = target_frames // current_frames + 1
                audio_features = np.tile(audio_features, (repeat_factor, 1))
                audio_features = audio_features[:target_frames]
            elif current_frames > target_frames:
                # å¦‚æœç‰¹å¾å¸§æ•°è¿‡å¤šï¼Œä½¿ç”¨æ’å€¼ä¸‹é‡‡æ ·
                indices = np.linspace(0, current_frames-1, target_frames).astype(int)
                audio_features = audio_features[indices] /21
            
            return audio_features  # shape: (750, hidden_size)
            
        except Exception as e:
            print(f"âš ï¸  WhisperéŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {audio_path} - {e}")
            return None

    def __len__(self):
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.mapping_df)

    def __getitem__(self, idx):
        """æŒ‰ç´¢å¼•è¯»å–æ•°æ®ï¼ˆLazy Loadingï¼‰- ç›´æ¥è¿”å›å®Œæ•´750å¸§æ ·æœ¬"""
        row = self.mapping_df.iloc[idx]
        speaker_parquet = self._convert_path(row.iloc[1], "parquet")
        listener_parquet = self._convert_path(row.iloc[2], "parquet")
        speaker_audio = self._convert_audio_path(row.iloc[1], "wav")
        listener_audio = self._convert_audio_path(row.iloc[2], "wav")

        # åŠ è½½é¢éƒ¨ç‰¹å¾æ•°æ®
        speaker_landmarks, speaker_au, speaker_pose, speaker_gaze = self._load_parquet(speaker_parquet)
        listener_landmarks, listener_au, listener_pose, listener_gaze = self._load_parquet(listener_parquet)
        
        # åŠ è½½éŸ³é¢‘ç‰¹å¾æ•°æ® (ä½¿ç”¨Whisper)
        speaker_audio_features = self._load_audio_with_whisper(speaker_audio)
        listener_audio_features = self._load_audio_with_whisper(listener_audio)

        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        face_data_valid = all(x is not None for x in [speaker_landmarks, speaker_au, speaker_pose, speaker_gaze,
                                                     listener_landmarks, listener_au, listener_pose, listener_gaze])
        audio_data_valid = all(x is not None for x in [speaker_audio_features, listener_audio_features])
        
        if not (face_data_valid and audio_data_valid):
            print(f"âš ï¸  è·³è¿‡ç´¢å¼• {idx}: æ•°æ®åŠ è½½å¤±è´¥")
            return None

        # ğŸ”¹ ç›´æ¥è¿”å›å®Œæ•´çš„750å¸§æ•°æ®ï¼Œä¸è¿›è¡Œåºåˆ—åˆ‡ç‰‡
        return {
            'speaker': {
                'landmarks': torch.FloatTensor(speaker_landmarks),     # (750, 136)
                'au': torch.FloatTensor(speaker_au),                  # (750, 18)
                'pose': torch.FloatTensor(speaker_pose),              # (750, 4)
                'gaze': torch.FloatTensor(speaker_gaze),              # (750, 2)
                'audio': torch.FloatTensor(speaker_audio_features)    # (750, 384)
            },
            'listener': {
                'landmarks': torch.FloatTensor(listener_landmarks),   # (750, 136)
                'au': torch.FloatTensor(listener_au),                # (750, 18)
                'pose': torch.FloatTensor(listener_pose),            # (750, 4)
                'gaze': torch.FloatTensor(listener_gaze),            # (750, 2)
                'audio': torch.FloatTensor(listener_audio_features)  # (750, 384)
            }
        }


def get_dataloader(mapping_csv, batch_size=2, num_workers=2, whisper_model_name="openai/whisper-tiny"):
    dataset = SpeakerListenerDataset(mapping_csv, whisper_model_name=whisper_model_name)
    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)


if __name__ == '__main__':
    # âœ… æŒ‡å®šæµ‹è¯•è·¯å¾„
    mapping_csv = "/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv"
    
    # âœ… åˆ›å»º DataLoader
    dataloader = get_dataloader(mapping_csv, batch_size=32, num_workers=0, whisper_model_name="openai/whisper-tiny")

    # âœ… å–å‡ºä¸€ä¸ª batch å¹¶æ£€æŸ¥å½¢çŠ¶
    for i, batch_data in enumerate(dataloader):
        if batch_data is None:
            continue
            
        print(f"ğŸŸ¢ Batch {i} Loaded Successfully!")

        # **æ£€æŸ¥Speakeré¢éƒ¨ç‰¹å¾æ•°æ® Shape**
        print(f"\nğŸ“Š Speaker Face Features:")
        print(f"  ğŸ—¿ Landmarks Shape: {batch_data['speaker']['landmarks'].shape}")  # (batch, 750, 136)
        print(f"  ğŸ˜Š AU Features Shape: {batch_data['speaker']['au'].shape}")      # (batch, 750, 18)
        print(f"  ğŸ“ Pose Features Shape: {batch_data['speaker']['pose'].shape}")  # (batch, 750, 4)
        print(f"  ğŸ‘ï¸ Gaze Features Shape: {batch_data['speaker']['gaze'].shape}")   # (batch, 750, 2)

        # **æ£€æŸ¥Listeneré¢éƒ¨ç‰¹å¾æ•°æ® Shape**
        print(f"\nğŸ“Š Listener Face Features:")
        print(f"  ğŸ—¿ Landmarks Shape: {batch_data['listener']['landmarks'].shape}")
        print(f"  ğŸ˜Š AU Features Shape: {batch_data['listener']['au'].shape}")
        print(f"  ğŸ“ Pose Features Shape: {batch_data['listener']['pose'].shape}")
        print(f"  ğŸ‘ï¸ Gaze Features Shape: {batch_data['listener']['gaze'].shape}")

        # **æ£€æŸ¥WhisperéŸ³é¢‘ç‰¹å¾æ•°æ® Shape**
        print(f"\nğŸµ Audio Features (Whisper):")
        print(f"  ğŸµ Speaker Audio Shape: {batch_data['speaker']['audio'].shape}")    # (batch, 750, whisper_hidden_size)
        print(f"  ğŸµ Listener Audio Shape: {batch_data['listener']['audio'].shape}")

        # **æ•°å€¼èŒƒå›´æ£€æŸ¥**
        print(f"\nğŸ“ˆ Data Range Check:")
        print(f"  ğŸ—¿ Landmarks range: [{batch_data['speaker']['landmarks'].min():.4f}, {batch_data['speaker']['landmarks'].max():.4f}]")
        print(f"  ğŸ˜Š AU range: [{batch_data['speaker']['au'].min():.4f}, {batch_data['speaker']['au'].max():.4f}]")
        print(f"  ğŸ“ Pose range: [{batch_data['speaker']['pose'].min():.4f}, {batch_data['speaker']['pose'].max():.4f}]")
        print(f"  ğŸ‘ï¸ Gaze range: [{batch_data['speaker']['gaze'].min():.4f}, {batch_data['speaker']['gaze'].max():.4f}]")
        print(f"  ğŸµ Audio range: [{batch_data['speaker']['audio'].min():.4f}, {batch_data['speaker']['audio'].max():.4f}]")

        # **æ£€æŸ¥åºåˆ—é•¿åº¦**
        print(f"\nğŸ“ Sequence Length Check:")
        print(f"  All sequences are 750 frames long: {batch_data['speaker']['landmarks'].shape[1]} frames")

        # **åªæµ‹è¯•ä¸€ä¸ª batch**
        break