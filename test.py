#!/usr/bin/env python3
"""
ç®€æ˜“æ¨¡å‹æµ‹è¯•è„šæœ¬
åŠ è½½checkpointï¼Œç”Ÿæˆé¢„æµ‹landmarksï¼Œä¿å­˜ç»“æœå¹¶å¯è§†åŒ–
"""

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from torch.utils.data import DataLoader
import argparse
from tqdm import tqdm

# å¯¼å…¥ä½ çš„æ¨¡å‹å’Œæ•°æ®é›†
from facial_reaction_model.model import FacialReactionModel
from Data_Set import SpeakerListenerDataset


class SimpleTester:
    """ç®€æ˜“æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.checkpoint_path = checkpoint_path
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {self.checkpoint_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # ä»checkpointè·å–æ¨¡å‹é…ç½®
        config = checkpoint.get('config', {})
        model_config = config.get('model', {})
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = FacialReactionModel(
            feature_dim=model_config.get('feature_dim', 256),
            audio_dim=model_config.get('audio_dim', 384),
            period=model_config.get('period', 25),
            max_seq_len=model_config.get('max_seq_len', 750),
            device=str(self.device),
            window_size=model_config.get('window_size', 16),
            momentum=model_config.get('momentum', 0.9)
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œepoch: {checkpoint.get('epoch', 'unknown')}")
        
    def load_test_sample(self, data_csv, sample_idx=0):
        """åŠ è½½å•ä¸ªæµ‹è¯•æ ·æœ¬"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {data_csv}, æ ·æœ¬ç´¢å¼•: {sample_idx}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SpeakerListenerDataset(data_csv)
        
        # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
        if sample_idx >= len(dataset):
            sample_idx = 0
            print(f"âš ï¸ æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨ç´¢å¼• 0")
        
        # è·å–æ ·æœ¬
        sample = dataset[sample_idx]
        if sample is None:
            raise ValueError(f"æ— æ³•åŠ è½½æ ·æœ¬ {sample_idx}")
        
        # è½¬æ¢ä¸ºbatchæ ¼å¼å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        batch_data = {}
        for role in ['speaker', 'listener']:
            batch_data[role] = {}
            for key, value in sample[role].items():
                batch_data[role][key] = value.unsqueeze(0).to(self.device)
        
        print(f"âœ… æµ‹è¯•æ ·æœ¬åŠ è½½æˆåŠŸ")
        print(f"   Speaker landmarks: {batch_data['speaker']['landmarks'].shape}")
        print(f"   Speaker AU: {batch_data['speaker']['au'].shape}")
        print(f"   Listener landmarks: {batch_data['listener']['landmarks'].shape}")
        print(f"   Listener AU: {batch_data['listener']['au'].shape}")
        
        return batch_data
    
    def generate_predictions(self, batch_data, num_samples=3):
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆé¢„æµ‹ (é‡‡æ ·æ¬¡æ•°: {num_samples})")
        
        speaker_data = batch_data['speaker']
        
        predictions_list = []
        
        with torch.no_grad():
            for i in range(num_samples):
                predictions, distributions, _ = self.model(speaker_data, speaker_out=False)
                
                # éªŒè¯é¢„æµ‹ç»“æœåŒ…å«æ‰€æœ‰å¿…è¦çš„ç‰¹å¾
                expected_features = ['landmarks', 'au', 'pose', 'gaze']
                for feature in expected_features:
                    if feature not in predictions:
                        raise ValueError(f"é¢„æµ‹ç»“æœç¼ºå°‘ç‰¹å¾: {feature}")
                
                predictions_list.append(predictions)
        
        print(f"âœ… é¢„æµ‹ç”Ÿæˆå®Œæˆ")
        
        # è¿”å›ç¬¬ä¸€æ¬¡é¢„æµ‹ä½œä¸ºä¸»è¦ç»“æœï¼Œä»¥åŠæ‰€æœ‰é¢„æµ‹ç”¨äºå¤šæ ·æ€§åˆ†æ
        return predictions_list[0], predictions_list
    
    def save_results(self, speaker_data, gt_listener, predictions, output_dir='test_results'):
        """ä¿å­˜é¢„æµ‹ç»“æœä¸ºCSV"""
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpyå¹¶å»æ‰batchç»´åº¦
        def to_numpy(tensor_dict):
            return {k: v.cpu().numpy().squeeze(0) for k, v in tensor_dict.items()}
        
        speaker_np = to_numpy(speaker_data)
        gt_listener_np = to_numpy(gt_listener)
        pred_listener_np = to_numpy(predictions)
        
        # åå½’ä¸€åŒ–æ‰€æœ‰ç‰¹å¾ (æ ¹æ®Data_Set.pyä¸­çš„å½’ä¸€åŒ–æ–¹å¼)
        def denormalize_features(data_dict):
            result = data_dict.copy()
            result['landmarks'] = data_dict['landmarks'] * 256.0  # landmarksé™¤ä»¥256å½’ä¸€åŒ–
            result['au'] = data_dict['au'] * 5.0                  # AUé™¤ä»¥5å½’ä¸€åŒ–
            # poseå’Œgazeä¿æŒåŸå§‹å€¼ï¼Œæ— éœ€åå½’ä¸€åŒ–
            return result
        
        speaker_denorm = denormalize_features(speaker_np)
        gt_denorm = denormalize_features(gt_listener_np)
        pred_denorm = denormalize_features(pred_listener_np)
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾ä¸ºCSV
        def save_all_features_csv(features_dict, prefix, output_dir):
            """ä¿å­˜æ‰€æœ‰ç‰¹å¾åˆ°CSVæ–‡ä»¶"""
            for feature_name, feature_data in features_dict.items():
                if feature_name == 'audio':  # éŸ³é¢‘ç‰¹å¾å¤ªå¤§ï¼Œè·³è¿‡
                    continue
                    
                filename = os.path.join(output_dir, f'{prefix}_{feature_name}.csv')
                
                if feature_name == 'landmarks':
                    # landmarksç‰¹æ®Šå¤„ç†ï¼šåˆ†ç¦»x,yåæ ‡
                    x_coords = feature_data[:, :68]
                    y_coords = feature_data[:, 68:]
                    columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
                    df = pd.DataFrame(feature_data, columns=columns)
                else:
                    # å…¶ä»–ç‰¹å¾ç›´æ¥ä¿å­˜
                    if feature_name == 'au':
                        columns = [f'AU_{i}' for i in range(feature_data.shape[1])]
                    elif feature_name == 'pose':
                        columns = ['pose_Rx', 'pose_Ry', 'pose_Rz']
                    elif feature_name == 'gaze':
                        columns = ['gaze_angle_x', 'gaze_angle_y']
                    else:
                        columns = [f'{feature_name}_{i}' for i in range(feature_data.shape[1])]
                    
                    df = pd.DataFrame(feature_data, columns=columns)
                
                df.insert(0, 'frame', range(len(df)))
                df.to_csv(filename, index=False)
        
        # ä¿å­˜æ‰€æœ‰ç‰¹å¾
        save_all_features_csv(speaker_denorm, 'speaker', output_dir)
        save_all_features_csv(gt_denorm, 'gt_listener', output_dir)
        save_all_features_csv(pred_denorm, 'predicted_listener', output_dir)
        
        # ä¸ºå‘åå…¼å®¹ï¼Œä»ç„¶è¿”å›landmarksçš„DataFrame
        def load_landmarks_df(features_dict):
            landmarks = features_dict['landmarks']
            x_coords = landmarks[:, :68]
            y_coords = landmarks[:, 68:]
            columns = [f'x_{i}' for i in range(68)] + [f'y_{i}' for i in range(68)]
            df = pd.DataFrame(landmarks, columns=columns)
            df.insert(0, 'frame', range(len(df)))
            return df
        
        speaker_df = load_landmarks_df(speaker_denorm)
        gt_df = load_landmarks_df(gt_denorm)
        pred_df = load_landmarks_df(pred_denorm)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {output_dir}:")
        print(f"   Speakerç‰¹å¾: speaker_landmarks.csv, speaker_au.csv, speaker_pose.csv, speaker_gaze.csv")
        print(f"   GT Listenerç‰¹å¾: gt_listener_landmarks.csv, gt_listener_au.csv, gt_listener_pose.csv, gt_listener_gaze.csv")
        print(f"   é¢„æµ‹Listenerç‰¹å¾: predicted_listener_landmarks.csv, predicted_listener_au.csv, predicted_listener_pose.csv, predicted_listener_gaze.csv")
        
        return speaker_df, gt_df, pred_df
    
    def create_comparison_animation(self, speaker_df, gt_df, pred_df, 
                                  output_path='comparison_animation.gif', max_frames=300):
        """åˆ›å»ºä¸‰è·¯å¯¹æ¯”åŠ¨ç”»"""
        print(f"ğŸ¬ åˆ›å»ºå¯¹æ¯”åŠ¨ç”»: {output_path}")
        
        # é™åˆ¶å¸§æ•°
        n_frames = min(len(speaker_df), len(gt_df), len(pred_df), max_frames)
        
        # æå–åæ ‡æ•°æ®
        def extract_coords(df):
            x_cols = [f'x_{i}' for i in range(68)]
            y_cols = [f'y_{i}' for i in range(68)]
            x_coords = df[x_cols].values[:n_frames]
            y_coords = df[y_cols].values[:n_frames]
            return x_coords, y_coords
        
        speaker_x, speaker_y = extract_coords(speaker_df)
        gt_x, gt_y = extract_coords(gt_df)
        pred_x, pred_y = extract_coords(pred_df)
        
        # è®¡ç®—åæ ‡èŒƒå›´
        all_x = np.concatenate([speaker_x.flatten(), gt_x.flatten(), pred_x.flatten()])
        all_y = np.concatenate([speaker_y.flatten(), gt_y.flatten(), pred_y.flatten()])
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        margin = 0.1 * max(x_max - x_min, y_max - y_min)
        
        # åˆ›å»ºä¸‰åˆ—å­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        def animate(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in [ax1, ax2, ax3]:
                ax.clear()
                ax.set_xlim(x_min - margin, x_max + margin)
                ax.set_ylim(y_max + margin, y_min - margin)  # ç¿»è½¬yè½´
                ax.set_aspect('equal')
            
            # è®¾ç½®æ ‡é¢˜
            ax1.set_title(f'Speaker - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax2.set_title(f'GT Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax3.set_title(f'Predicted Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            
            # ç»˜åˆ¶ç‰¹å¾ç‚¹
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7)
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7)
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7)
            
            # ç»˜åˆ¶åŸºæœ¬è¿æ¥çº¿
            for ax, x_coords, y_coords in [(ax1, speaker_x[frame], speaker_y[frame]),
                                         (ax2, gt_x[frame], gt_y[frame]),
                                         (ax3, pred_x[frame], pred_y[frame])]:
                self.draw_face_connections(ax, x_coords, y_coords)
        
        # åˆ›å»ºåŠ¨ç”»
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, 
                                     interval=100, repeat=True)
        
        # ä¿å­˜
        anim.save(output_path, writer='pillow', fps=10, dpi=80)
        plt.close()
        
        print(f"âœ… åŠ¨ç”»å·²ä¿å­˜åˆ°: {output_path}")
    
    def draw_face_connections(self, ax, x_coords, y_coords):
        """ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿"""
        # ç®€åŒ–çš„é¢éƒ¨è¿æ¥çº¿
        connections = {
            'jaw': list(range(0, 17)),           # ä¸‹é¢Œçº¿
            'right_eyebrow': list(range(17, 22)), # å³çœ‰æ¯›
            'left_eyebrow': list(range(22, 27)),  # å·¦çœ‰æ¯›
            'nose_bridge': list(range(27, 31)),   # é¼»æ¢
            'nose_tip': list(range(31, 36)),      # é¼»ç¿¼
            'right_eye': list(range(36, 42)) + [36], # å³çœ¼
            'left_eye': list(range(42, 48)) + [42],  # å·¦çœ¼
            'outer_lip': list(range(48, 60)) + [48], # å¤–å˜´å”‡
            'inner_lip': list(range(60, 68)) + [60]  # å†…å˜´å”‡
        }
        
        colors = ['brown', 'orange', 'orange', 'red', 'red', 'purple', 'purple', 'pink', 'pink']
        
        for i, (name, indices) in enumerate(connections.items()):
            if len(indices) > 1:
                x_line = [x_coords[idx] for idx in indices]
                y_line = [y_coords[idx] for idx in indices]
                ax.plot(x_line, y_line, color=colors[i % len(colors)], 
                       linewidth=1, alpha=0.6)
    
    def calculate_metrics(self, gt_listener, predictions):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        metrics = {}
        
        # å¯¹æ¯ç§ç‰¹å¾è®¡ç®—æŒ‡æ ‡
        for feature_name in ['landmarks', 'au', 'pose', 'gaze']:
            gt_data = gt_listener[feature_name].cpu().numpy().squeeze(0)
            pred_data = predictions[feature_name].cpu().numpy().squeeze(0)
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            mse = np.mean((gt_data - pred_data) ** 2)
            mae = np.mean(np.abs(gt_data - pred_data))
            rmse = np.sqrt(mse)
            
            metrics[feature_name] = {
                'mse': mse,
                'mae': mae,
                'rmse': rmse
            }
            
            print(f"  {feature_name.upper()}:")
            print(f"    MSE: {mse:.6f}")
            print(f"    MAE: {mae:.6f}")
            print(f"    RMSE: {rmse:.6f}")
        
        # å¯¹landmarksè®¡ç®—æ›´è¯¦ç»†çš„æŒ‡æ ‡
        if 'landmarks' in metrics:
            gt_landmarks = gt_listener['landmarks'].cpu().numpy().squeeze(0)
            pred_landmarks = predictions['landmarks'].cpu().numpy().squeeze(0)
            
            # åˆ†ç¦»xå’Œyåæ ‡çš„è¯¯å·®
            x_errors = np.abs(gt_landmarks[:, :68] - pred_landmarks[:, :68])
            y_errors = np.abs(gt_landmarks[:, 68:] - pred_landmarks[:, 68:])
            
            metrics['landmarks']['x_mae'] = np.mean(x_errors)
            metrics['landmarks']['y_mae'] = np.mean(y_errors)
            metrics['landmarks']['max_point_error'] = np.max(np.sqrt(x_errors**2 + y_errors**2))
            
            print(f"    Xåæ ‡MAE: {metrics['landmarks']['x_mae']:.6f}")
            print(f"    Yåæ ‡MAE: {metrics['landmarks']['y_mae']:.6f}")
            print(f"    æœ€å¤§ç‚¹è¯¯å·®: {metrics['landmarks']['max_point_error']:.6f}")
        
        return metrics
    
    def run_test(self, data_csv, sample_idx=0, output_dir='test_results'):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        
        # 1. åŠ è½½æµ‹è¯•æ ·æœ¬
        batch_data = self.load_test_sample(data_csv, sample_idx)
        
        # 2. ç”Ÿæˆé¢„æµ‹
        predictions, all_predictions = self.generate_predictions(batch_data)
        
        # 3. è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(batch_data['listener'], predictions)
        
        # 4. ä¿å­˜ç»“æœ
        speaker_df, gt_df, pred_df = self.save_results(
            batch_data['speaker'], 
            batch_data['listener'], 
            predictions, 
            output_dir
        )
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        animation_path = os.path.join(output_dir, 'comparison_animation.gif')
        self.create_comparison_animation(speaker_df, gt_df, pred_df, animation_path)
        
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return metrics, animation_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç®€æ˜“æ¨¡å‹æµ‹è¯•")
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data-csv', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv',
                       help='æµ‹è¯•æ•°æ®CSVè·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='æµ‹è¯•æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--output-dir', type=str, default='test_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥checkpointæ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data_csv):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_csv}")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SimpleTester(args.checkpoint, args.device)
        
        # è¿è¡Œæµ‹è¯•
        metrics, animation_path = tester.run_test(
            args.data_csv, 
            args.sample_idx, 
            args.output_dir
        )
        
        print("\n" + "="*50)
        print("ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*50)
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"åŠ¨ç”»æ–‡ä»¶: {animation_path}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()