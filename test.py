#!/usr/bin/env python3
"""
ç®€æ˜“æ¨¡å‹æµ‹è¯•è„šæœ¬
åŠ è½½checkpointï¼Œç”Ÿæˆé¢„æµ‹landmarksï¼Œä¿å­˜ç»“æœå¹¶å¯è§†åŒ–
"""

import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from torch.utils.data import DataLoader
import argparse
from tqdm import tqdm

# å¯¼å…¥ä½ çš„æ¨¡å‹å’Œæ•°æ®é›†
from facial_reaction_model.model import FacialReactionModel
from Data_Set import SpeakerListenerDataset


class SimpleTester:
    """ç®€æ˜“æµ‹è¯•å™¨"""
    
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.checkpoint_path = checkpoint_path
        
        # åŠ è½½æ¨¡å‹
        self.load_model()
        
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹ä»: {self.checkpoint_path}")
        
        # åŠ è½½checkpoint
        checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
        
        # ä»checkpointè·å–æ¨¡å‹é…ç½®
        config = checkpoint.get('config', {})
        model_config = config.get('model', {})
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        self.model = FacialReactionModel(
            feature_dim=model_config.get('feature_dim', 256),
            audio_dim=model_config.get('audio_dim', 384),
            period=model_config.get('period', 25),
            max_seq_len=model_config.get('max_seq_len', 750),
            device=str(self.device),
            window_size=model_config.get('window_size', 16),
            momentum=model_config.get('momentum', 0.9)
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œepoch: {checkpoint.get('epoch', 'unknown')}")
        
    def load_test_sample(self, data_csv, sample_idx=0):
        """åŠ è½½å•ä¸ªæµ‹è¯•æ ·æœ¬"""
        print(f"ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®: {data_csv}, æ ·æœ¬ç´¢å¼•: {sample_idx}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = SpeakerListenerDataset(data_csv)
        
        # æ£€æŸ¥ç´¢å¼•æœ‰æ•ˆæ€§
        if sample_idx >= len(dataset):
            sample_idx = 0
            print(f"âš ï¸ æ ·æœ¬ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨ç´¢å¼• 0")
        
        # è·å–æ ·æœ¬
        sample = dataset[sample_idx]
        if sample is None:
            raise ValueError(f"æ— æ³•åŠ è½½æ ·æœ¬ {sample_idx}")
        
        # è½¬æ¢ä¸ºbatchæ ¼å¼å¹¶ç§»åŠ¨åˆ°è®¾å¤‡
        batch_data = {}
        for role in ['speaker', 'listener']:
            batch_data[role] = {}
            for key, value in sample[role].items():
                batch_data[role][key] = value.unsqueeze(0).to(self.device)
        
        print(f"âœ… æµ‹è¯•æ ·æœ¬åŠ è½½æˆåŠŸ")
        print(f"   Speaker landmarks: {batch_data['speaker']['landmarks'].shape}")
        print(f"   Listener landmarks: {batch_data['listener']['landmarks'].shape}")
        
        return batch_data
    
    def generate_predictions(self, batch_data, num_samples=3):
        """ç”Ÿæˆé¢„æµ‹ç»“æœ"""
        print(f"ğŸ¯ å¼€å§‹ç”Ÿæˆé¢„æµ‹ (é‡‡æ ·æ¬¡æ•°: {num_samples})")
        
        speaker_data = batch_data['speaker']
        
        predictions_list = []
        
        with torch.no_grad():
            for i in range(num_samples):
                predictions, distributions, _ = self.model(speaker_data, speaker_out=False)
                predictions_list.append(predictions)
        
        print(f"âœ… é¢„æµ‹ç”Ÿæˆå®Œæˆ")
        
        # è¿”å›ç¬¬ä¸€æ¬¡é¢„æµ‹ä½œä¸ºä¸»è¦ç»“æœï¼Œä»¥åŠæ‰€æœ‰é¢„æµ‹ç”¨äºå¤šæ ·æ€§åˆ†æ
        return predictions_list[0], predictions_list
    
    def save_results(self, speaker_data, gt_listener, predictions, output_dir='test_results'):
        """ä¿å­˜é¢„æµ‹ç»“æœä¸ºCSV"""
        os.makedirs(output_dir, exist_ok=True)
        
        # è½¬æ¢ä¸ºnumpyå¹¶å»æ‰batchç»´åº¦
        def to_numpy(tensor_dict):
            return {k: v.cpu().numpy().squeeze(0) for k, v in tensor_dict.items()}
        
        speaker_np = to_numpy(speaker_data)
        gt_listener_np = to_numpy(gt_listener)
        pred_listener_np = to_numpy(predictions)
        
        # åå½’ä¸€åŒ–landmarks (ä¹˜ä»¥256ï¼Œå› ä¸ºè®­ç»ƒæ—¶é™¤ä»¥256å½’ä¸€åŒ–)
        def denormalize_landmarks(data_dict):
            result = data_dict.copy()
            result['landmarks'] = data_dict['landmarks'] * 256.0
            return result
        
        speaker_denorm = denormalize_landmarks(speaker_np)
        gt_denorm = denormalize_landmarks(gt_listener_np)
        pred_denorm = denormalize_landmarks(pred_listener_np)
        
        # ä¿å­˜ä¸ºCSV
        def save_landmarks_csv(landmarks, filename):
            # landmarks shape: (750, 136) -> (750, 68*2)
            # åˆ†ç¦»xå’Œyåæ ‡
            x_coords = landmarks[:, :68]  # å‰68åˆ—æ˜¯x
            y_coords = landmarks[:, 68:]  # å68åˆ—æ˜¯y
            
            # åˆ›å»ºDataFrame
            columns = []
            for i in range(68):
                columns.append(f'x_{i}')
            for i in range(68):
                columns.append(f'y_{i}')
            
            df = pd.DataFrame(landmarks, columns=columns)
            df.insert(0, 'frame', range(len(df)))
            df.to_csv(filename, index=False)
            return df
        
        # ä¿å­˜æ‰€æœ‰ç»“æœ
        speaker_df = save_landmarks_csv(speaker_denorm['landmarks'], 
                                      os.path.join(output_dir, 'speaker_landmarks.csv'))
        gt_df = save_landmarks_csv(gt_denorm['landmarks'], 
                                 os.path.join(output_dir, 'gt_listener_landmarks.csv'))
        pred_df = save_landmarks_csv(pred_denorm['landmarks'], 
                                   os.path.join(output_dir, 'predicted_listener_landmarks.csv'))
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {output_dir}:")
        print(f"   - speaker_landmarks.csv")
        print(f"   - gt_listener_landmarks.csv") 
        print(f"   - predicted_listener_landmarks.csv")
        
        return speaker_df, gt_df, pred_df
    
    def create_comparison_animation(self, speaker_df, gt_df, pred_df, 
                                  output_path='comparison_animation.gif', max_frames=300):
        """åˆ›å»ºä¸‰è·¯å¯¹æ¯”åŠ¨ç”»"""
        print(f"ğŸ¬ åˆ›å»ºå¯¹æ¯”åŠ¨ç”»: {output_path}")
        
        # é™åˆ¶å¸§æ•°
        n_frames = min(len(speaker_df), len(gt_df), len(pred_df), max_frames)
        
        # æå–åæ ‡æ•°æ®
        def extract_coords(df):
            x_cols = [f'x_{i}' for i in range(68)]
            y_cols = [f'y_{i}' for i in range(68)]
            x_coords = df[x_cols].values[:n_frames]
            y_coords = df[y_cols].values[:n_frames]
            return x_coords, y_coords
        
        speaker_x, speaker_y = extract_coords(speaker_df)
        gt_x, gt_y = extract_coords(gt_df)
        pred_x, pred_y = extract_coords(pred_df)
        
        # è®¡ç®—åæ ‡èŒƒå›´
        all_x = np.concatenate([speaker_x.flatten(), gt_x.flatten(), pred_x.flatten()])
        all_y = np.concatenate([speaker_y.flatten(), gt_y.flatten(), pred_y.flatten()])
        x_min, x_max = np.min(all_x), np.max(all_x)
        y_min, y_max = np.min(all_y), np.max(all_y)
        margin = 0.1 * max(x_max - x_min, y_max - y_min)
        
        # åˆ›å»ºä¸‰åˆ—å­å›¾
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        def animate(frame):
            # æ¸…é™¤æ‰€æœ‰å­å›¾
            for ax in [ax1, ax2, ax3]:
                ax.clear()
                ax.set_xlim(x_min - margin, x_max + margin)
                ax.set_ylim(y_max + margin, y_min - margin)  # ç¿»è½¬yè½´
                ax.set_aspect('equal')
            
            # è®¾ç½®æ ‡é¢˜
            ax1.set_title(f'Speaker - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax2.set_title(f'GT Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            ax3.set_title(f'Predicted Listener - å¸§ {frame+1}/{n_frames}', fontsize=12)
            
            # ç»˜åˆ¶ç‰¹å¾ç‚¹
            ax1.scatter(speaker_x[frame], speaker_y[frame], c='blue', s=15, alpha=0.7)
            ax2.scatter(gt_x[frame], gt_y[frame], c='green', s=15, alpha=0.7)
            ax3.scatter(pred_x[frame], pred_y[frame], c='red', s=15, alpha=0.7)
            
            # ç»˜åˆ¶åŸºæœ¬è¿æ¥çº¿
            for ax, x_coords, y_coords in [(ax1, speaker_x[frame], speaker_y[frame]),
                                         (ax2, gt_x[frame], gt_y[frame]),
                                         (ax3, pred_x[frame], pred_y[frame])]:
                self.draw_face_connections(ax, x_coords, y_coords)
        
        # åˆ›å»ºåŠ¨ç”»
        anim = animation.FuncAnimation(fig, animate, frames=n_frames, 
                                     interval=100, repeat=True)
        
        # ä¿å­˜
        anim.save(output_path, writer='pillow', fps=10, dpi=80)
        plt.close()
        
        print(f"âœ… åŠ¨ç”»å·²ä¿å­˜åˆ°: {output_path}")
    
    def draw_face_connections(self, ax, x_coords, y_coords):
        """ç»˜åˆ¶é¢éƒ¨è¿æ¥çº¿"""
        # ç®€åŒ–çš„é¢éƒ¨è¿æ¥çº¿
        connections = {
            'jaw': list(range(0, 17)),           # ä¸‹é¢Œçº¿
            'right_eyebrow': list(range(17, 22)), # å³çœ‰æ¯›
            'left_eyebrow': list(range(22, 27)),  # å·¦çœ‰æ¯›
            'nose_bridge': list(range(27, 31)),   # é¼»æ¢
            'nose_tip': list(range(31, 36)),      # é¼»ç¿¼
            'right_eye': list(range(36, 42)) + [36], # å³çœ¼
            'left_eye': list(range(42, 48)) + [42],  # å·¦çœ¼
            'outer_lip': list(range(48, 60)) + [48], # å¤–å˜´å”‡
            'inner_lip': list(range(60, 68)) + [60]  # å†…å˜´å”‡
        }
        
        colors = ['brown', 'orange', 'orange', 'red', 'red', 'purple', 'purple', 'pink', 'pink']
        
        for i, (name, indices) in enumerate(connections.items()):
            if len(indices) > 1:
                x_line = [x_coords[idx] for idx in indices]
                y_line = [y_coords[idx] for idx in indices]
                ax.plot(x_line, y_line, color=colors[i % len(colors)], 
                       linewidth=1, alpha=0.6)
    
    def calculate_metrics(self, gt_listener, predictions):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # è½¬æ¢ä¸ºnumpy
        gt_landmarks = gt_listener['landmarks'].cpu().numpy().squeeze(0)
        pred_landmarks = predictions['landmarks'].cpu().numpy().squeeze(0)
        
        # è®¡ç®—MSE, MAE, RMSE
        mse = np.mean((gt_landmarks - pred_landmarks) ** 2)
        mae = np.mean(np.abs(gt_landmarks - pred_landmarks))
        rmse = np.sqrt(mse)
        
        # è®¡ç®—æ¯ä¸ªç‰¹å¾ç‚¹çš„å¹³å‡è¯¯å·®
        point_errors = np.mean(np.abs(gt_landmarks - pred_landmarks), axis=0)
        
        # åˆ†ç¦»xå’Œyåæ ‡çš„è¯¯å·®
        x_errors = point_errors[:68]
        y_errors = point_errors[68:]
        
        print(f"è¯„ä¼°æŒ‡æ ‡:")
        print(f"  MSE: {mse:.6f}")
        print(f"  MAE: {mae:.6f}")
        print(f"  RMSE: {rmse:.6f}")
        print(f"  å¹³å‡Xè¯¯å·®: {np.mean(x_errors):.6f}")
        print(f"  å¹³å‡Yè¯¯å·®: {np.mean(y_errors):.6f}")
        
        return {
            'mse': mse,
            'mae': mae,
            'rmse': rmse,
            'x_error': np.mean(x_errors),
            'y_error': np.mean(y_errors)
        }
    
    def run_test(self, data_csv, sample_idx=0, output_dir='test_results'):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ¨¡å‹æµ‹è¯•...")
        
        # 1. åŠ è½½æµ‹è¯•æ ·æœ¬
        batch_data = self.load_test_sample(data_csv, sample_idx)
        
        # 2. ç”Ÿæˆé¢„æµ‹
        predictions, all_predictions = self.generate_predictions(batch_data)
        
        # 3. è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(batch_data['listener'], predictions)
        
        # 4. ä¿å­˜ç»“æœ
        speaker_df, gt_df, pred_df = self.save_results(
            batch_data['speaker'], 
            batch_data['listener'], 
            predictions, 
            output_dir
        )
        
        # 5. åˆ›å»ºå¯è§†åŒ–
        animation_path = os.path.join(output_dir, 'comparison_animation.gif')
        self.create_comparison_animation(speaker_df, gt_df, pred_df, animation_path)
        
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        return metrics, animation_path


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç®€æ˜“æ¨¡å‹æµ‹è¯•")
    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--data-csv', type=str, 
                       default='/mnt/iusers01/fatpou01/compsci01/k09562zs/scratch/Face_data/val.csv',
                       help='æµ‹è¯•æ•°æ®CSVè·¯å¾„')
    parser.add_argument('--sample-idx', type=int, default=0,
                       help='æµ‹è¯•æ ·æœ¬ç´¢å¼•')
    parser.add_argument('--output-dir', type=str, default='test_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥checkpointæ–‡ä»¶
    if not os.path.exists(args.checkpoint):
        print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {args.checkpoint}")
        return
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(args.data_csv):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_csv}")
        return
    
    try:
        # åˆ›å»ºæµ‹è¯•å™¨
        tester = SimpleTester(args.checkpoint, args.device)
        
        # è¿è¡Œæµ‹è¯•
        metrics, animation_path = tester.run_test(
            args.data_csv, 
            args.sample_idx, 
            args.output_dir
        )
        
        print("\n" + "="*50)
        print("ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“")
        print("="*50)
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        print(f"åŠ¨ç”»æ–‡ä»¶: {animation_path}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()